{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mfrdixon/ML_Finance_Codes/blob/master/Wealth_Management_GIRL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vc1tLT83PQMd"
   },
   "outputs": [],
   "source": [
    "# ML_in_Finance-GIRL-wealth-management\n",
    "# Author: Igor Halperin and Matthew Dixon\n",
    "# Version: 1.0 (12.8.2019)\n",
    "# License: MIT\n",
    "# Email: ighalp@gmail.com\n",
    "# Notes: tested on Mac OS X with Python 3.6 and PyTorch 1.3.0\n",
    "# Citation: Please cite the following reference if this notebook is used for research purposes:\n",
    "# Dixon M.F., I. Halperin and P. Bilokon, Machine Learning in Finance: From Theory to Practice, Springer Graduate textbook Series, 2020. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uXdeHDmEPQM6"
   },
   "source": [
    "## G-learing and GIRL for wealth optimization\n",
    "\n",
    "This notebook demonstrates the application of G-learning and GIRL for optimization of a defined contribution retirement plan. The notebook extends the G-learning notebook in Chapter 10 with an example of applying GIRL to infer the parameters of the G-learner used to generate the trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vOoE16hWPQNC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "from scipy.optimize import minimize\n",
    "import torch\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "M-6MadDnPQNO",
    "outputId": "176e0a88-271e-4be2-9d69-1f13e1520534"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PHyhUTpJ4nl1",
    "outputId": "18b1c78b-d6c6-4453-9755-65390364ab53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: KMP_DUPLICATE_LIB_OK=TRUE\n"
     ]
    }
   ],
   "source": [
    "%env KMP_DUPLICATE_LIB_OK=TRUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S0yKeMU9PQNa"
   },
   "outputs": [],
   "source": [
    "# set the device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qLivr9yuPQON"
   },
   "outputs": [],
   "source": [
    "# Define the G-learning portfolio optimization class\n",
    "class G_learning_portfolio_opt:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 num_steps,\n",
    "                 params,\n",
    "                 beta,\n",
    "                 benchmark_portf,\n",
    "                 gamma, \n",
    "                 num_risky_assets,\n",
    "                 riskfree_rate,\n",
    "                 exp_returns, # array of shape num_steps x num_stocks\n",
    "                 Sigma_r,     # covariance matrix of returns of risky assets\n",
    "                 init_x_vals, # array of initial asset position values (num_risky_assets + 1)\n",
    "                 use_for_WM = True): # use for wealth management tasks\n",
    "\n",
    "                \n",
    "        self.num_steps = num_steps\n",
    "        self.num_assets = num_risky_assets + 1 # exp_returns.shape[1]\n",
    "        \n",
    "        self.lambd = torch.tensor(params[0], requires_grad=False, dtype=torch.float64)\n",
    "        self.Omega_mat = params[1] * torch.eye(self.num_assets,dtype=torch.float64)\n",
    "        #self.Omega_mat = torch.tensor(Omega_mat,requires_grad=False, dtype=torch.float64)\n",
    "        self.eta = torch.tensor(params[2], requires_grad=False, dtype=torch.float64)\n",
    "        self.rho = torch.tensor(params[3], requires_grad=False, dtype=torch.float64)\n",
    "        self.beta = torch.tensor(beta, requires_grad=False, dtype=torch.float64)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.use_for_WM = use_for_WM\n",
    "        \n",
    "        self.num_risky_assets = num_risky_assets\n",
    "        self.r_f = riskfree_rate\n",
    "        \n",
    "        \n",
    "        assert exp_returns.shape[0] == self.num_steps\n",
    "        assert Sigma_r.shape[0] == Sigma_r.shape[1]\n",
    "        assert Sigma_r.shape[0] == num_risky_assets # self.num_assets\n",
    "        \n",
    "        self.Sigma_r_np = Sigma_r # array of shape num_stocks x num_stocks\n",
    "        \n",
    "        self.reg_mat = 1e-3*torch.eye(self.num_assets, dtype=torch.float64)\n",
    "        \n",
    "        # arrays of returns for all assets including the risk-free asset\n",
    "        # array of shape num_steps x (num_stocks + 1) \n",
    "        self.exp_returns_np = np.hstack((self.r_f * np.ones(self.num_steps).reshape((-1,1)), exp_returns))\n",
    "                                      \n",
    "        # make block-matrix Sigma_r_tilde with Sigma_r_tilde[0,0] = 0, and equity correlation matrix inside\n",
    "        self.Sigma_r_tilde_np = np.zeros((self.num_assets, self.num_assets))\n",
    "        self.Sigma_r_tilde_np[1:,1:] = self.Sigma_r_np\n",
    "            \n",
    "        # make Torch tensors  \n",
    "        self.exp_returns = torch.tensor(self.exp_returns_np,requires_grad=False, dtype=torch.float64)\n",
    "        self.Sigma_r = torch.tensor(Sigma_r,requires_grad=False, dtype=torch.float64)\n",
    "        self.Sigma_r_tilde = torch.tensor(self.Sigma_r_tilde_np,requires_grad=False, dtype=torch.float64)\n",
    "        \n",
    "        self.benchmark_portf = torch.tensor(benchmark_portf, requires_grad=False, dtype=torch.float64)\n",
    "        \n",
    "        # asset holding values for all times. Initialize with initial values, \n",
    "        # values for the future times will be expected values \n",
    "        self.x_vals_np = np.zeros((self.num_steps, self.num_assets))\n",
    "        self.x_vals_np[0,:] = init_x_vals \n",
    "        \n",
    "        # Torch tensor\n",
    "        self.x_vals = torch.tensor(self.x_vals_np)\n",
    "                \n",
    "        # allocate memory for coefficients of R-, F- and G-functions        \n",
    "        self.F_xx = torch.zeros(self.num_steps, self.num_assets, self.num_assets, dtype=torch.float64,\n",
    "                                requires_grad=True)\n",
    "        self.F_x = torch.zeros(self.num_steps, self.num_assets, dtype=torch.float64,\n",
    "                               requires_grad=True)\n",
    "        self.F_0 = torch.zeros(self.num_steps,dtype=torch.float64,requires_grad=True)\n",
    "        \n",
    "        self.Q_xx = torch.zeros(self.num_steps, self.num_assets, self.num_assets,dtype=torch.float64,\n",
    "                                requires_grad=True)\n",
    "        self.Q_uu = torch.zeros(self.num_steps, self.num_assets, self.num_assets,dtype=torch.float64,\n",
    "                                requires_grad=True)\n",
    "        self.Q_ux = torch.zeros(self.num_steps, self.num_assets, self.num_assets,dtype=torch.float64,\n",
    "                                requires_grad=True)\n",
    "        self.Q_x = torch.zeros(self.num_steps, self.num_assets,dtype=torch.float64,requires_grad=True)\n",
    "        self.Q_u = torch.zeros(self.num_steps, self.num_assets,dtype=torch.float64,requires_grad=True)\n",
    "        self.Q_0 = torch.zeros(self.num_steps,dtype=torch.float64,requires_grad=True)\n",
    "        \n",
    "        self.R_xx = torch.zeros(self.num_steps, self.num_assets, self.num_assets,dtype=torch.float64,\n",
    "                                requires_grad=True)\n",
    "        self.R_uu = torch.zeros(self.num_steps, self.num_assets, self.num_assets,dtype=torch.float64,\n",
    "                                requires_grad=True)\n",
    "        self.R_ux = torch.zeros(self.num_steps, self.num_assets, self.num_assets,dtype=torch.float64,\n",
    "                                requires_grad=True)\n",
    "        self.R_x = torch.zeros(self.num_steps, self.num_assets,dtype=torch.float64,requires_grad=True)\n",
    "        self.R_u = torch.zeros(self.num_steps, self.num_assets,dtype=torch.float64,requires_grad=True)\n",
    "        self.R_0 = torch.zeros(self.num_steps,dtype=torch.float64,requires_grad=True)\n",
    "\n",
    "        \n",
    "        self.reset_prior_policy()\n",
    "        \n",
    "        # the list of adjustable model parameters:\n",
    "        self.model_params = [self.lambd, self.beta, self.Omega_mat, self.eta]  \n",
    "#                              self.exp_returns, self.Sigma_r_tilde,self.Sigma_prior_inv, self.u_bar_prior]\n",
    "        \n",
    "        \n",
    "        # expected cash installment for all steps\n",
    "        self.expected_c_t = torch.zeros(self.num_steps,dtype=torch.float64)\n",
    "        \n",
    "        # realized values of the target portfolio\n",
    "        self.realized_target_portf = np.zeros(self.num_steps,dtype=np.float64)\n",
    "        \n",
    "        # expected portfolio values for all times\n",
    "        self.expected_portf_val = torch.zeros(self.num_steps,dtype=torch.float64)\n",
    "        \n",
    "        # the first value is the sum of initial position values\n",
    "        self.expected_portf_val[0] = self.x_vals[0,:].sum()\n",
    "\n",
    "    def reset_prior_policy(self):\n",
    "        # initialize time-dependent parameters of prior policy \n",
    "        self.u_bar_prior = torch.zeros(self.num_steps,self.num_assets,requires_grad=False,\n",
    "                                       dtype=torch.float64)\n",
    "        self.v_bar_prior =  torch.zeros(self.num_steps, self.num_assets, self.num_assets,requires_grad=False,\n",
    "                                        dtype=torch.float64)\n",
    "        self.Sigma_prior =  torch.zeros(self.num_steps, self.num_assets, self.num_assets,requires_grad=False,\n",
    "                                        dtype=torch.float64)\n",
    "        self.Sigma_prior_inv = torch.zeros(self.num_steps, self.num_assets, self.num_assets,requires_grad=False,\n",
    "                                        dtype=torch.float64)\n",
    "        \n",
    "        # make each time elements of v_bar_prior and Sigma_prior proportional to the unit matrix\n",
    "        for t in range(self.num_steps):\n",
    "            self.v_bar_prior[t,:,:] = 0.1 * torch.eye(self.num_assets).clone()\n",
    "            self.Sigma_prior[t,:,:] = 0.1 * torch.eye(self.num_assets).clone()\n",
    "            self.Sigma_prior_inv[t,:,:] = 10.0 * torch.eye(self.num_assets).clone() # np.linalg.inv(self.Sigma_prior[t,:,:])\n",
    "    \n",
    "    def reward_fun(self, t, x_vals, u_vals, exp_rets, lambd, Sigma_hat):\n",
    "        \"\"\"\n",
    "        The reward function \n",
    "        \"\"\"\n",
    "        x_plus = x_vals + u_vals\n",
    "        \n",
    "        p_hat = self.rho.clone() * self.benchmark_portf[t] + (1-self.rho.clone())*self.eta.clone()*x_vals.sum()\n",
    "        \n",
    "        aux_1 = - self.lambd.clone() * p_hat**2         \n",
    "        aux_2 = - u_vals.sum()   \n",
    "        aux_3 = 2*self.lambd.clone() * p_hat * x_plus.dot(torch.ones(num_assets) + exp_rets)\n",
    "        aux_4 = - self.lambd.clone() * x_plus.mm(Sigma_hat.mv(x_plus))\n",
    "        aux_5 = - u_vals.mm(self.Omega_mat.clone().mv(u_vals))\n",
    "        \n",
    "        return aux_1 + aux_2 + aux_3 + aux_4 + aux_5  \n",
    "    \n",
    "    def compute_reward_fun(self):\n",
    "        \"\"\"\n",
    "        Compute coefficients R_xx, R_ux, etc. for all steps\n",
    "        \"\"\"\n",
    "        for t in range(0, self.num_steps):\n",
    "            \n",
    "            one_plus_exp_ret = torch.ones(self.num_assets,dtype=torch.float64) + self.exp_returns[t,:]\n",
    "            benchmark_portf = self.benchmark_portf[t]\n",
    "            Sigma_hat = self.Sigma_r_tilde + torch.ger(one_plus_exp_ret, one_plus_exp_ret)\n",
    "            \n",
    "            one_plus_exp_ret_by_one = torch.ger(one_plus_exp_ret,torch.ones(self.num_assets,dtype=torch.float64))\n",
    "            one_plus_exp_ret_by_one_T = one_plus_exp_ret_by_one.t()     \n",
    "            one_one_T_mat = torch.ones(self.num_assets,self.num_assets)\n",
    "            \n",
    "            self.R_xx[t,:,:] = (-self.lambd.clone()*(self.eta.clone()**2)*(self.rho.clone()**2)*one_one_T_mat\n",
    "                                 + 2*self.lambd.clone()*self.eta.clone()*self.rho.clone()*one_plus_exp_ret_by_one\n",
    "                                 - self.lambd.clone()*Sigma_hat)\n",
    "            \n",
    "            self.R_ux[t,:,:] = (2*self.lambd.clone()*self.eta.clone()*self.rho.clone()*one_plus_exp_ret_by_one\n",
    "                                 - 2*self.lambd.clone()*Sigma_hat)\n",
    "            \n",
    "            self.R_uu[t,:,:] = - self.lambd.clone() * Sigma_hat - self.Omega_mat.clone()\n",
    "            \n",
    "            self.R_x[t,:] =  (-2*self.lambd.clone()*self.eta.clone()*self.rho.clone()*(1-self.rho.clone())*benchmark_portf *\n",
    "                                 torch.ones(self.num_assets,dtype=torch.float64)\n",
    "                                 + 2*self.lambd.clone()*(1-self.rho.clone())*benchmark_portf * one_plus_exp_ret)\n",
    "            \n",
    "            self.R_u[t,:] = (2*self.lambd.clone()*(1-self.rho.clone())*benchmark_portf * one_plus_exp_ret\n",
    "                             - torch.ones(self.num_assets,dtype=torch.float64))\n",
    "            \n",
    "            self.R_0[t] = - self.lambd.clone()*((1-self.rho.clone())**2) * (benchmark_portf**2)\n",
    "                \n",
    "         \n",
    "    def project_cash_injections(self):\n",
    "        \"\"\"\n",
    "        Compute the expected values of future asset positions, and the expected cash injection for future steps,\n",
    "        as well as realized values of the target portfolio\n",
    "        \"\"\"\n",
    "           \n",
    "        # this assumes that the policy is trained\n",
    "        for t in range(1, self.num_steps):  # the initial value is fixed \n",
    "            \n",
    "            # increment the previous x_t\n",
    "            \n",
    "            delta_x_t = self.u_bar_prior[t,:] + self.v_bar_prior[t,:,:].mv(self.x_vals[t-1,:])\n",
    "            self.x_vals[t,:] = self.x_vals[t-1,:] + delta_x_t\n",
    "            \n",
    "            # grow using the expected return\n",
    "            self.x_vals[t,:] = (torch.ones(self.num_assets)+ self.exp_returns[t,:])*self.x_vals[t,:]\n",
    "            \n",
    "            # compute c_t\n",
    "            self.expected_c_t[t] = delta_x_t.sum().data # detach().numpy()\n",
    "            \n",
    "            # expected portfolio value for this step\n",
    "            self.expected_portf_val[t] = self.x_vals[t,:].sum().data # .detach().numpy()\n",
    "    \n",
    "            \n",
    "                                                                                      \n",
    "    def set_terminal_conditions(self):\n",
    "        \"\"\"\n",
    "        set the terminal condition for the F-function\n",
    "        \"\"\"\n",
    "        \n",
    "        # the auxiliary quantity to perform matrix calculations\n",
    "        one_plus_exp_ret = torch.ones(self.num_assets,dtype=torch.float64) + self.exp_returns[-1,:]\n",
    "        \n",
    "        \n",
    "        # Compute the reward function for all steps (only the last step is needed for this functions, while \n",
    "        # values for other time steps will be used in other functions)\n",
    "        self.compute_reward_fun()\n",
    "        \n",
    "        if self.use_for_WM:\n",
    "\n",
    "            Sigma_hat = self.Sigma_r_tilde + torch.ger(one_plus_exp_ret, one_plus_exp_ret)\n",
    "            Sigma_hat_inv = torch.inverse(Sigma_hat + self.reg_mat)\n",
    "            \n",
    "            Sigma_tilde = Sigma_hat + (1/self.lambd)*self.Omega_mat.clone()\n",
    "            #Sigma_tilde_inv = torch.pinverse(Sigma_tilde)\n",
    "            Sigma_tilde_inv = torch.inverse(Sigma_tilde + self.reg_mat)\n",
    "            \n",
    "            Sigma_hat_sigma_tilde = Sigma_hat.mm(Sigma_tilde)\n",
    "            Sigma_tilde_inv_sig_hat = Sigma_tilde_inv.mm(Sigma_hat)\n",
    "            Sigma_tilde_sigma_hat = Sigma_tilde.mm(Sigma_hat)\n",
    "            \n",
    "            Sigma_hat_Sigma_tilde_inv = Sigma_hat.mm(Sigma_tilde_inv)\n",
    "            Sigma_3_plus_omega = self.lambd*Sigma_tilde_inv.mm(Sigma_hat_Sigma_tilde_inv) + self.Omega_mat.clone()    \n",
    "                             \n",
    "            one_plus_exp_ret_by_one = torch.ger(one_plus_exp_ret,torch.ones(self.num_assets,dtype=torch.float64))\n",
    "            one_plus_exp_ret_by_one_T = one_plus_exp_ret_by_one.t()     \n",
    "            one_one_T_mat = torch.ones(self.num_assets,self.num_assets)\n",
    "            \n",
    "            Sigma_tilde_inv_t_R_ux = Sigma_tilde_inv.t().mm(self.R_ux[-1,:,:].clone())\n",
    "            Sigma_tilde_inv_t_R_uu = Sigma_tilde_inv.t().mm(self.R_uu[-1,:,:].clone())\n",
    "            Sigma_tilde_inv_t_R_u = Sigma_tilde_inv.t().mv(self.R_u[-1,:].clone())\n",
    "            \n",
    "            Sigma_tilde_inv_R_u = Sigma_tilde_inv.mv(self.R_u[-1,:].clone())\n",
    "            Sigma_tilde_inv_R_ux = Sigma_tilde_inv.mm(self.R_ux[-1,:,:].clone())\n",
    "            Sigma_tilde_inv_t_R_uu = Sigma_tilde_inv.mm(self.R_uu[-1,:,:].clone())\n",
    "            \n",
    "            # though the action at the last step is deterministic, we can feed \n",
    "            # parameters of the prior with these values                     \n",
    "              \n",
    "            self.u_bar_prior[-1,:]   = (1/(2 * self.lambd.clone()))* Sigma_tilde_inv.clone().mv(self.R_u[-1,:].clone())\n",
    "            self.v_bar_prior[-1,:,:] = (1/(2 * self.lambd.clone()))* Sigma_tilde_inv.clone().mm(self.R_ux[-1,:,:].clone())    \n",
    "                \n",
    "            # First compute the coefficients of the reward function at the last step\n",
    "\n",
    "            \n",
    "            # the coefficients of F-function for the last step\n",
    "            \n",
    "            # F_xx                 \n",
    "            self.F_xx[-1,:,:] = (self.R_xx[-1,:,:].clone()\n",
    "                                 + (1/(2*self.lambd.clone()))* self.R_ux[-1,:,:].clone().t().mm(Sigma_tilde_inv_t_R_ux)\n",
    "                                 + (1/(4*self.lambd.clone()**2))* self.R_ux[-1,:,:].clone().t().mm(\n",
    "                                      Sigma_tilde_inv_t_R_uu.clone().mm(Sigma_tilde_inv.clone().mm(self.R_ux[-1,:,:].clone())))\n",
    "                                )\n",
    "            \n",
    "            # F_x                    \n",
    "            self.F_x[-1,:] = (self.R_x[-1,:].clone()\n",
    "                                 + (1/(self.lambd.clone()))* self.R_ux[-1,:,:].clone().t().mv(Sigma_tilde_inv_t_R_u.clone())\n",
    "                                 + (1/(2*self.lambd.clone()**2))* self.R_ux[-1,:,:].clone().t().mv(\n",
    "                                      Sigma_tilde_inv_t_R_uu.clone().mv(Sigma_tilde_inv_R_u.clone()))\n",
    "                            )\n",
    "            \n",
    "            \n",
    "        \n",
    "            # F_0   \n",
    "            self.F_0[-1] = (self.R_0[-1].clone() \n",
    "                            +  (1/(2*self.lambd.clone()))* self.R_u[-1,:].clone().dot(Sigma_tilde_inv_R_u.clone())\n",
    "                            + (1/(4*self.lambd.clone()**2))* self.R_u[-1,:].clone().dot(\n",
    "                                Sigma_tilde_inv_t_R_uu.clone().mv(Sigma_tilde_inv_R_u.clone()))\n",
    "                           )\n",
    "            \n",
    "            # for the Q-function at the last step:\n",
    "            self.Q_xx[-1,:,:] = self.R_xx[-1,:,:].clone()\n",
    "            self.Q_ux[-1,:,:] = self.R_ux[-1,:,:].clone()\n",
    "            self.Q_uu[-1,:,:] = self.R_uu[-1,:,:].clone()\n",
    "            self.Q_u[-1,:] = self.R_u[-1,:].clone()\n",
    "            self.Q_x[-1,:] = self.R_x[-1,:].clone()\n",
    "            self.Q_0[-1] = self.R_0[-1].clone()\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "                \n",
    "    def G_learning(self, err_tol, max_iter):\n",
    "        \"\"\"\n",
    "        find the optimal policy for the time dependent policy\n",
    "        \n",
    "        \"\"\"   \n",
    "        print('Doing G-learning, it may take a few seconds...')\n",
    "        \n",
    "        # set terminal conditions\n",
    "        self.set_terminal_conditions()\n",
    "        \n",
    "        # allocate iteration numbers for all steps\n",
    "        self.iter_counts = np.zeros(self.num_steps)\n",
    "        \n",
    "        # iterate over time steps backward\n",
    "        for t in range(self.num_steps-2,-1,-1):\n",
    "            self.step_G_learning(t, err_tol, max_iter)\n",
    "            \n",
    "    def step_G_learning(self, t, err_tol, max_iter):\n",
    "        \"\"\"\n",
    "        Perform one step of backward iteration for G-learning self-consistent equations\n",
    "        This should start from step t = num_steps - 2 (i.e. from a step that is before the last one)\n",
    "        \"\"\"\n",
    "            \n",
    "        # make matrix Sigma_hat_t        \n",
    "        one_plus_exp_ret = torch.ones(self.num_assets,dtype=torch.float64) + self.exp_returns[t,:]\n",
    "        Sigma_hat_t = self.Sigma_r_tilde + torch.ger(one_plus_exp_ret, one_plus_exp_ret)\n",
    "        \n",
    "        # matrix A_t = diag(1 + r_bar_t)\n",
    "        A_t = torch.diag(torch.ones(self.num_assets,dtype=torch.float64) + self.exp_returns[t,:])\n",
    "                    \n",
    "        # update parameters of Q_function using next-step F-function values\n",
    "        self.update_Q_params(t, A_t,Sigma_hat_t)\n",
    "             \n",
    "        # iterate between policy evaluation and policy improvement  \n",
    "        while self.iter_counts[t] < max_iter:\n",
    "                \n",
    "            curr_u_bar_prior = self.u_bar_prior[t,:].clone()  \n",
    "            curr_v_bar_prior = self.v_bar_prior[t,:,:].clone()     \n",
    "                \n",
    "            # compute parameters of F-function for this step from parameters of Q-function\n",
    "            self.update_F_params(t) \n",
    "              \n",
    "            # Policy iteration step: update parameters of the prior policy distribution\n",
    "            # with given Q- and F-function parameters\n",
    "            self.update_policy_params(t)    \n",
    "            \n",
    "            # difference between the current value of u_bar_prior and the previous one\n",
    "            err_u_bar = torch.sum((curr_u_bar_prior - self.u_bar_prior[t,:])**2)\n",
    "            \n",
    "            # divide by num_assets in err_v_bar to get both errors on a comparable scale\n",
    "            err_v_bar = (1/self.num_assets)*torch.sum((curr_v_bar_prior - self.v_bar_prior[t,:,:])**2)\n",
    "            \n",
    "            # choose the difference from the previous iteration as the maximum of the two errors\n",
    "            tol = torch.max(err_u_bar, err_v_bar)  # tol = 0.5*(err_u_bar + err_v_bar)\n",
    "            \n",
    "            self.iter_counts[t] += 1\n",
    "            # Repeat the calculation of Q- and F-values\n",
    "            if tol <= err_tol:\n",
    "                break\n",
    "                \n",
    "            \n",
    "    def update_Q_params(self,t, A_t,Sigma_hat_t):\n",
    "        \"\"\"\n",
    "        update the current (time-t) parameters of Q-function from (t+1)-parameters of F-function\n",
    "        \"\"\" \n",
    "                \n",
    "        ones = torch.ones(self.num_assets,dtype=torch.float64)    \n",
    "        one_plus_exp_ret = torch.ones(self.num_assets,dtype=torch.float64) + self.exp_returns[t,:]\n",
    "    \n",
    "        self.Q_xx[t,:,:] = (self.R_xx[t,:,:].clone() \n",
    "                            + self.gamma *( (A_t.clone().mm(self.F_xx[t+1,:,:].clone())).mm(A_t.clone())  \n",
    "                                           + self.Sigma_r_tilde.clone() * self.F_xx[t+1,:,:].clone() ) )\n",
    "\n",
    "\n",
    "        self.Q_ux[t,:,:] = (self.R_ux[t,:,:].clone() \n",
    "                            + 2 * self.gamma *( (A_t.clone().mm(self.F_xx[t+1,:,:].clone())).mm(A_t.clone())  \n",
    "                                           + self.Sigma_r_tilde.clone() * self.F_xx[t+1,:,:].clone() ) \n",
    "                           )\n",
    "    \n",
    "        self.Q_uu[t,:,:] = (self.R_uu[t,:,:].clone()  \n",
    "                            + self.gamma *( (A_t.clone().mm(self.F_xx[t+1,:,:].clone())).mm(A_t.clone())  \n",
    "                                           + self.Sigma_r_tilde.clone() * self.F_xx[t+1,:,:].clone() )\n",
    "                            - self.Omega_mat.clone()\n",
    "                           )\n",
    "\n",
    "\n",
    "        self.Q_x[t,:] = self.R_x[t,:].clone() + self.gamma * A_t.clone().mv(self.F_x[t+1,:].clone()) \n",
    "        self.Q_u[t,:] = self.R_u[t,:].clone() + self.gamma * A_t.clone().mv(self.F_x[t+1,:].clone())\n",
    "        self.Q_0[t]   = self.R_0[t].clone() + self.gamma * self.F_0[t+1].clone()\n",
    "\n",
    "\n",
    "        \n",
    "    def update_F_params(self,t):\n",
    "        \"\"\"\n",
    "        update the current (time-t) parameters of F-function from t-parameters of G-function\n",
    "        This is a policy evaluation step: it uses the current estimations of the mean parameters of the policy\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # produce auxiliary parameters U_t, W_t, Sigma_tilde_t\n",
    "        U_t = (self.beta.clone() * self.Q_ux[t,:,:].clone() \n",
    "               + self.Sigma_prior_inv[t,:,:].clone().mm(self.v_bar_prior[t,:,:].clone()))\n",
    "        W_t = (self.beta.clone() * self.Q_u[t,:].clone() \n",
    "               +  self.Sigma_prior_inv[t,:,:].clone().mv(self.u_bar_prior[t,:]).clone())\n",
    "        Sigma_p_bar =  self.Sigma_prior_inv[t,:,:].clone() - 2 * self.beta.clone() * self.Q_uu[t,:,:].clone()\n",
    "        Sigma_p_bar_inv = torch.inverse(Sigma_p_bar + self.reg_mat)\n",
    "        \n",
    "        # update parameters of F-function\n",
    "        self.F_xx[t,:,:] = self.Q_xx[t,:,:].clone() + (1/(2*self.beta.clone()))*(U_t.t().mm(Sigma_p_bar_inv.clone().mm(U_t))\n",
    "                                    - self.v_bar_prior[t,:,:].clone().t().mm(\n",
    "                                        self.Sigma_prior_inv[t,:,:].clone().mm(self.v_bar_prior[t,:,:].clone())))\n",
    "        \n",
    "        \n",
    "        self.F_x[t,:] = self.Q_x[t,:].clone() + (1/self.beta.clone())*(U_t.mv(Sigma_p_bar_inv.clone().mv(W_t))\n",
    "                                    - self.v_bar_prior[t,:,:].clone().mv(\n",
    "                                        self.Sigma_prior_inv[t,:,:].clone().mv(self.u_bar_prior[t,:].clone())))\n",
    "        \n",
    "        \n",
    "        self.F_0[t] = self.Q_0[t].clone() + ( (1/(2*self.beta.clone()))*(W_t.dot(Sigma_p_bar_inv.clone().mv(W_t))\n",
    "                                    - self.u_bar_prior[t,:].clone().dot(\n",
    "                                        self.Sigma_prior_inv[t,:,:].clone().mv(self.u_bar_prior[t,:].clone())))\n",
    "                                    - (1/(2*self.beta.clone())) * (torch.log(torch.det(self.Sigma_prior[t,:,:].clone()+\n",
    "                                                                              self.reg_mat))\n",
    "                                                       - torch.log(torch.det(Sigma_p_bar_inv.clone() + self.reg_mat))) )\n",
    "        \n",
    "        \n",
    "        \n",
    "    def update_policy_params(self,t):\n",
    "        \"\"\"\n",
    "        update parameters of the Gaussian policy using current coefficients of the F- and G-functions\n",
    "        \"\"\"\n",
    "        \n",
    "        new_Sigma_prior_inv = self.Sigma_prior_inv[t,:,:].clone() - 2 * self.beta.clone() * self.Q_uu[t,:,:].clone()\n",
    "        \n",
    "        Sigma_prior_new = torch.inverse(new_Sigma_prior_inv + self.reg_mat)\n",
    "        \n",
    "        # update parameters using the previous value of Sigma_prior_inv\n",
    "        self.u_bar_prior[t,:] = Sigma_prior_new.mv(self.Sigma_prior_inv[t,:,:].clone().mv(self.u_bar_prior[t,:].clone())\n",
    "                                              + self.beta.clone() * self.Q_u[t,:].clone())\n",
    "        \n",
    "        \n",
    "        self.v_bar_prior[t,:,:] = Sigma_prior_new.clone().mm(self.Sigma_prior_inv[t,:,:].clone().mm(self.v_bar_prior[t,:,:].clone())\n",
    "                                              + self.beta.clone() * self.Q_ux[t,:,:].clone())\n",
    "        \n",
    "        # and then assign the new inverse covariance for the prior for the next iteration\n",
    "        self.Sigma_prior[t,:,:] = Sigma_prior_new.clone()\n",
    "        self.Sigma_prior_inv[t,:,:] = new_Sigma_prior_inv.clone()\n",
    "        \n",
    "        # also assign the same values for the previous time step\n",
    "        if t > 0:\n",
    "            self.Sigma_prior[t-1,:,:] = self.Sigma_prior[t,:,:].clone()\n",
    "            self.u_bar_prior[t-1,:] = self.u_bar_prior[t,:].clone()\n",
    "            self.v_bar_prior[t-1,:,:] = self.v_bar_prior[t,:,:].clone()\n",
    "            \n",
    "    def trajs_to_torch_tensors(self,trajs):\n",
    "        \"\"\"\n",
    "        Convert data from a list of lists into Torch tensors\n",
    "        \"\"\"\n",
    "        num_trajs = len(trajs)\n",
    "        \n",
    "        self.data_xvals = torch.zeros(num_trajs,self.num_steps,self.num_assets,dtype=torch.float64)\n",
    "        self.data_uvals = torch.zeros(num_trajs,self.num_steps,self.num_assets,dtype=torch.float64)\n",
    "            \n",
    "        for n in range(num_trajs):\n",
    "            for t in range(self.num_steps):\n",
    "                self.data_xvals[n,t,:] = torch.tensor(trajs[n][t][0],dtype=torch.float64).clone()\n",
    "                self.data_uvals[n,t,:] = torch.tensor(trajs[n][t][1],dtype=torch.float64).clone()\n",
    "                \n",
    "    def compute_reward_on_traj(self,\n",
    "                              t,\n",
    "                              x_t, u_t):\n",
    "        \"\"\"\n",
    "        Given time t and corresponding values of vectors x_t, u_t, compute the total reward for this step\n",
    "        \"\"\"\n",
    "        \n",
    "        aux_xx = x_t.dot(self.R_xx[t,:,:].clone().mv(x_t))\n",
    "        aux_ux = u_t.dot(self.R_ux[t,:,:].clone().mv(x_t))\n",
    "        aux_uu = u_t.dot(self.R_uu[t,:,:].clone().mv(u_t))\n",
    "        aux_x = x_t.dot(self.R_x[t,:].clone())\n",
    "        aux_u = u_t.dot(self.R_u[t,:].clone())\n",
    "        aux_0 = self.R_0[t].clone()\n",
    "        \n",
    "        return aux_xx + aux_ux + aux_uu + aux_x + aux_u + aux_0\n",
    "    \n",
    "    def compute_G_fun_on_traj(self,\n",
    "                              t,\n",
    "                              x_t, u_t):\n",
    "        \"\"\"\n",
    "        Given time t and corresponding values of vectors x_t, u_t, compute the total reward for this step\n",
    "        \"\"\"\n",
    "        \n",
    "        aux_xx = x_t.dot(self.Q_xx[t,:,:].clone().mv(x_t))\n",
    "        aux_ux = u_t.dot(self.Q_ux[t,:,:].clone().mv(x_t))\n",
    "        aux_uu = u_t.dot(self.Q_uu[t,:,:].clone().mv(u_t))\n",
    "        aux_x = x_t.dot(self.Q_x[t,:].clone())\n",
    "        aux_u = u_t.dot(self.Q_u[t,:].clone())\n",
    "        aux_0 = self.Q_0[t].clone()\n",
    "        \n",
    "        return aux_xx + aux_ux + aux_uu + aux_x + aux_u + aux_0\n",
    "    \n",
    "    def compute_F_fun_on_traj(self,\n",
    "                              t,\n",
    "                              x_t):\n",
    "        \"\"\"\n",
    "        Given time t and corresponding values of vectors x_t, u_t, compute the total reward for this step\n",
    "        \"\"\"\n",
    "        \n",
    "        aux_xx = x_t.dot(self.F_xx[t,:,:].clone().mv(x_t))\n",
    "        aux_x = x_t.dot(self.F_x[t,:].clone())\n",
    "        aux_0 = self.F_0[t].clone()\n",
    "        \n",
    "        return aux_xx + aux_x + aux_0\n",
    "\n",
    "                 \n",
    "    def MaxEntIRL(self,\n",
    "                  trajs,\n",
    "                  learning_rate,\n",
    "                  err_tol, max_iter):\n",
    "        \n",
    "        \"\"\"\n",
    "        Estimate parameters of the reward function using MaxEnt IRL.\n",
    "        Inputs:\n",
    "        \n",
    "        trajs - a list of trajectories. Each trajectory is a list of state-action pairs, stored as a tuple.\n",
    "                We assume each trajectory has the same length\n",
    "        \"\"\"\n",
    "        \n",
    "        # omega is a tunable parameter that determines the cost matrix self.Omega_mat\n",
    "        omega_init = 15.0\n",
    "        self.omega = torch.tensor(omega_init, requires_grad=True, dtype=torch.float64)\n",
    "        \n",
    "        # also set beta to be small \n",
    "        beta_init = 50 \n",
    "        self.beta = torch.tensor(beta_init, requires_grad=True, dtype=torch.float64)\n",
    "        \n",
    "        reward_params =  [self.lambd, self.eta, self.rho, self.omega, self.beta]\n",
    "        \n",
    "        print(\"Omega mat...\")\n",
    "        self.Omega_mat = self.omega * torch.eye(self.num_assets,dtype=torch.float64)\n",
    "        print(\"g learning...\")\n",
    "        self.reset_prior_policy()\n",
    "        self.G_learning(err_tol, max_iter)\n",
    "        print(\"intialize optimizer...\")\n",
    "        optimizer = optim.Adam(reward_params, lr=learning_rate)\n",
    "        print(\"zero grad...\")\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        num_trajs = len(trajs)\n",
    "        print(\"trajs_to_torch_tensors...\")\n",
    "        # fill in Torch tensors for the trajectory data\n",
    "        self.trajs_to_torch_tensors(trajs)\n",
    "        print(\"constructing zero tensors...\")   \n",
    "        self.realized_rewards = torch.zeros(num_trajs,self.num_steps,dtype=torch.float64,requires_grad=True)\n",
    "        self.realized_cum_rewards = torch.zeros(num_trajs, dtype=torch.float64, requires_grad=True)\n",
    "        print(\"constructing zero tensors...\")  \n",
    "        self.realized_G_fun = torch.zeros(num_trajs,self.num_steps,dtype=torch.float64, requires_grad=True)\n",
    "        self.realized_F_fun = torch.zeros(num_trajs,self.num_steps,dtype=torch.float64, requires_grad=True)\n",
    "        print(\"constructing zero tensors...\")  \n",
    "        self.realized_G_fun_cum = torch.zeros(num_trajs,dtype=torch.float64, requires_grad=True)\n",
    "        self.realized_F_fun_cum = torch.zeros(num_trajs,dtype=torch.float64, requires_grad=True)\n",
    "        print(\"done...\")  \n",
    "        \n",
    "        num_iter_IRL = 3\n",
    "        \n",
    "        for i in range(num_iter_IRL):\n",
    "            \n",
    "            print('GIRL iteration = ', i)\n",
    "       \n",
    "            self.Omega_mat = self.omega * torch.eye(self.num_assets,dtype=torch.float64)\n",
    "    \n",
    "            for n in range(101):\n",
    "                if n%100==0:\n",
    "                    print(n)\n",
    "                for t in range(self.num_steps):\n",
    "                    \n",
    "                    \n",
    "                    # compute rewards obtained at each step for each trajectory\n",
    "                    # given the model paramaters\n",
    "        \n",
    "                    self.realized_rewards[n,t] = self.compute_reward_on_traj(t,\n",
    "                                                                self.data_xvals[n,t,:],\n",
    "                                                                self.data_uvals[n,t,:])\n",
    "                                                                \n",
    "            \n",
    "                    # compute the log-likelihood by looping over trajectories\n",
    "                    self.realized_G_fun[n,t] = self.compute_G_fun_on_traj(t,\n",
    "                                                                self.data_xvals[n,t,:],\n",
    "                                                                self.data_uvals[n,t,:])\n",
    "                \n",
    "                \n",
    "                    self.realized_F_fun[n,t] = self.compute_F_fun_on_traj(t,\n",
    "                                                                self.data_xvals[n,t,:])\n",
    "                \n",
    "\n",
    "                self.realized_cum_rewards[n] = self.realized_rewards[n,:].sum().clone()\n",
    "                self.realized_G_fun_cum[n] = self.realized_G_fun[n,:].sum().clone()\n",
    "                self.realized_F_fun_cum[n] = self.realized_F_fun[n,:].sum().clone()\n",
    "            \n",
    "            # the negative log-likelihood will not include terms ~ Sigma_p as we do not optimize over its value\n",
    "            loss = - self.beta.clone()*(self.realized_G_fun_cum.sum().clone() - self.realized_F_fun_cum.sum().clone())\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            loss.backward() \n",
    "        \n",
    "            optimizer.step()\n",
    "        \n",
    "            print('Iteration = ', i)\n",
    "            print('Loss = ', loss.detach().numpy())\n",
    "        \n",
    "           \n",
    "        print('Done optimizing reward parameters')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N_8XeDlpPQOV"
   },
   "source": [
    "## Simulate portfolio data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gy2u0fkyPQOX"
   },
   "source": [
    "### Simulate the market factor under a lognormal distribution with a fixed drift and vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F25fZ-s9PQOb"
   },
   "outputs": [],
   "source": [
    "mu_market = 0.05\n",
    "vol_market = 0.25\n",
    "init_market_val = 100.0\n",
    "\n",
    "r_rf = 0.02  # risk-free rate - the first asset will be cash\n",
    "\n",
    "num_steps = 10\n",
    "dt = 0.25 # quarterly time steps\n",
    "\n",
    "num_risky_assets = 99 \n",
    "\n",
    "returns_market = np.zeros(num_steps)\n",
    "market_vals = np.zeros(num_steps)\n",
    "market_vals[0] = 100.0  # initial value\n",
    "\n",
    "\n",
    "        \n",
    "for t in range(1,num_steps):\n",
    "\n",
    "        rand_norm = np.random.randn()\n",
    "        \n",
    "        # use log-returns of market as 'returns_market'\n",
    "        returns_market[t] = mu_market * dt + vol_market * np.sqrt(dt) * rand_norm\n",
    "        \n",
    "        market_vals[t] = market_vals[t-1] * np.exp((mu_market - 0.5*vol_market**2)*dt + \n",
    "                                                         vol_market*np.sqrt(dt)*rand_norm)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sUUBRbwbPQO1"
   },
   "source": [
    "### Simulate market betas and idiosyncratic alphas within pre-defined ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "qraR7fRVPQO4",
    "outputId": "dfb260b8-fa17-4a70-a12d-86a4c2964e3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.38888481 0.23879842 0.08635828 0.56004803 0.06598472 0.75617181\n",
      " 0.34627799 0.74072757 0.79658177 0.24476291]\n",
      "[0.01616581 0.00395208 0.06559046 0.08333434 0.07694703 0.06277679\n",
      " 0.10888204 0.10132533 0.12329485 0.10018176]\n"
     ]
    }
   ],
   "source": [
    "beta_min = 0.05\n",
    "beta_max = 0.85\n",
    "beta_vals = np.random.uniform(low=beta_min, high=beta_max, size=num_risky_assets)\n",
    "\n",
    "alpha_min = - 0.05\n",
    "alpha_max = 0.15\n",
    "alpha_vals = np.random.uniform(low=alpha_min, high=alpha_max, size=num_risky_assets)\n",
    "\n",
    "print(beta_vals[0:10])\n",
    "print(alpha_vals[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DNFzG5dgPQPB"
   },
   "source": [
    "### Simulate time-dependent expected returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a36GRMAlPQPD"
   },
   "outputs": [],
   "source": [
    "# Time-independent expected returns would be equal to alpha + beta * expected_market_return \n",
    "# Make them time-dependent (and correlated with actual returns) as alpha + beta * oracle_market_returns\n",
    "\n",
    "oracle_coeff = 0.2\n",
    "mu_vec = mu_market * np.ones(num_steps)\n",
    "oracle_market_returns = mu_vec * dt + oracle_coeff*(returns_market - mu_vec)\n",
    "\n",
    "expected_risky_returns = np.zeros((num_steps, num_risky_assets))\n",
    "\n",
    "for t in range(num_steps):\n",
    "    expected_risky_returns[t,:] = alpha_vals * dt + beta_vals * oracle_market_returns[t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DDN_R3znPQPK"
   },
   "source": [
    "### Initial values of all assets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YneTj6IBPQPM"
   },
   "outputs": [],
   "source": [
    "val_min = 20.0\n",
    "val_max = 120.0\n",
    "\n",
    "init_risky_asset_vals = np.random.uniform(low=val_min, high=val_max, size=num_risky_assets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nqd7WIxbPQPS"
   },
   "source": [
    "### Simulate realized returns and asset prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tSL2TU1IPQPW"
   },
   "outputs": [],
   "source": [
    "# Generate realized returns and realized asset values by simulating from a one-factor model \n",
    "# with time-dependent expected returns\n",
    "\n",
    "risky_asset_returns = np.zeros((num_steps, num_risky_assets))\n",
    "risky_asset_vals = np.zeros((num_steps, num_risky_assets))\n",
    "\n",
    "idiosync_vol =  0.05 # vol_market \n",
    "\n",
    "for t in range(num_steps):\n",
    "    \n",
    "    rand_norm = np.random.randn(num_risky_assets)\n",
    "        \n",
    "    # asset returns are simulated from a one-factor model\n",
    "    risky_asset_returns[t,:] = (expected_risky_returns[t,:] + beta_vals * (returns_market[t] - mu_market * dt) \n",
    "                         + idiosync_vol * np.sqrt(1 - beta_vals**2) * np.sqrt(dt) * rand_norm)\n",
    "        \n",
    "    # asset values\n",
    "    if t == 0:\n",
    "        risky_asset_vals[t,:] = init_risky_asset_vals\n",
    "    else:\n",
    "        risky_asset_vals[t] = risky_asset_vals[t-1] * (1 + risky_asset_returns[t,:])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "YvHvV7B6PQPh",
    "outputId": "3421ed28-a65f-4362-d166-990d22dbf1fd"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare the realized and expected returns\n",
    "# Note that they should appear correlated.\n",
    "\n",
    "# pick a random asset ID to show\n",
    "asset_idx =  4 \n",
    "\n",
    "plt.plot(expected_risky_returns[:,asset_idx],label='expected_return')\n",
    "plt.plot(risky_asset_returns[:,asset_idx],label='realized_return',color='r')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Steps')\n",
    "plt.title('Realized returns vs expected returns')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WqAdjcoFPQPp"
   },
   "source": [
    "### Compute the empirical correlation matrix using realized returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sl5JQ3_8PQPs",
    "outputId": "5f93d784-2e83-4a2c-bf2e-23bd42eb5fb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99, 99)\n"
     ]
    }
   ],
   "source": [
    "cov_mat_r = np.cov(risky_asset_returns.T) \n",
    "\n",
    "print(cov_mat_r.shape)\n",
    "\n",
    "D,v = np.linalg.eigh(cov_mat_r)\n",
    "\n",
    "eigenvals = D[::-1]  # put them in a descended order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "H-HzoeHOPQP0",
    "outputId": "96c11580-037d-430d-fe77-5e54635ea519"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.18438721e-01, 7.82418568e-03, 6.04329219e-03, 5.85191397e-03,\n",
       "       4.77084015e-03, 4.44418054e-03, 4.25298938e-03, 3.96754172e-03,\n",
       "       3.53635223e-03, 4.63010281e-17])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eigenvalues: the largest eigenvalue is the market factor \n",
    "eigenvals[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "hFC61Ci94nnd",
    "outputId": "fc1d8cde-9ada-4e39-f6e6-3fe3b55559e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 33.9064,   9.3252,   9.4684,  ..., -12.5213,   9.5298,   8.1695],\n",
       "        [  9.3252,  39.1112,   9.4735,  ..., -15.4381,   9.9311,  -7.9070],\n",
       "        [  9.4684,   9.4735,   9.2873,  ...,  -4.1432,   2.4905,  -0.9692],\n",
       "        ...,\n",
       "        [-12.5213, -15.4381,  -4.1432,  ...,  29.9866,   5.9177,   0.3197],\n",
       "        [  9.5298,   9.9311,   2.4905,  ...,   5.9177,  15.1844,   2.0105],\n",
       "        [  8.1695,  -7.9070,  -0.9692,  ...,   0.3197,   2.0105,  10.4434]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov_mat_torch = torch.tensor(cov_mat_r)\n",
    "\n",
    "torch.pinverse(cov_mat_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ohSC6oDePQP5"
   },
   "source": [
    "### Add a riskless bond as one more asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F7nwaM28PQP9"
   },
   "outputs": [],
   "source": [
    "num_assets = num_risky_assets + 1\n",
    "\n",
    "bond_val = 100.0\n",
    "\n",
    "# add the bond to initial assets\n",
    "init_asset_vals = np.hstack((np.array([bond_val]),\n",
    "                            init_risky_asset_vals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hZQgn_CTPQQE"
   },
   "source": [
    "### Make the initial portfolio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PyMLBotBPQQH"
   },
   "outputs": [],
   "source": [
    "# consider here two choices: equal or equally-weighted \n",
    "\n",
    "init_port_choice =  'equal' \n",
    "\n",
    "init_cash = 1000.0\n",
    "init_total_asset = np.sum(init_asset_vals)\n",
    "\n",
    "x_vals_init = np.zeros(num_assets)\n",
    "\n",
    "if init_port_choice == 'equal': \n",
    "    # hold equal amounts of cash in each asset\n",
    "    amount_per_asset = init_cash/num_assets\n",
    "    x_vals_init = amount_per_asset * np.ones(num_assets)\n",
    "\n",
    "elif init_port_choice == 'equally_weighted':\n",
    "    amount_per_asset = init_cash/init_total_asset\n",
    "    x_vals_init = amount_per_asset * init_asset_vals\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zkj8GdPDPQQP"
   },
   "source": [
    "### Make the target portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LyyKCaPGPQQS",
    "outputId": "bda0d8d5-b848-4c2b-c36b-3f2ac1a1bf78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1100.0 1541.5835692311712\n"
     ]
    }
   ],
   "source": [
    "# make a target portfolio term structure by defining it as the initial portfolio growing at some fixed and high rate\n",
    "\n",
    "target_portfolio = [init_cash]\n",
    "\n",
    "target_return = 0.15 \n",
    "coeff_target = 1.1 \n",
    "\n",
    "for i in range(1,num_steps):\n",
    "    target_portfolio.append(target_portfolio[i-1]*np.exp(dt * target_return) )\n",
    "    \n",
    "target_portfolio = coeff_target*np.array(target_portfolio)    \n",
    "print(target_portfolio[0], target_portfolio[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2UMA8UwlPQQc"
   },
   "source": [
    "### Define model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5G_Y87m2PQQe",
    "outputId": "0cf27618-1f16-4676-9741-4e2c3ce8d466"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1133.1484530668263 3490.3429574618413\n"
     ]
    }
   ],
   "source": [
    "riskfree_rate = 0.02\n",
    "fee_bond = 0.05 \n",
    "fee_stock = 0.05 \n",
    "\n",
    "all_fees = np.zeros(num_risky_assets + 1)\n",
    "all_fees[0] = fee_bond\n",
    "all_fees[1:] = fee_stock\n",
    "Omega_mat = np.diag(all_fees)\n",
    "\n",
    "\n",
    "# model parameters\n",
    "\n",
    "lambd = 0.001 \n",
    "Omega_mat = 15.5 * np.diag(all_fees) \n",
    "eta = 1.5 \n",
    "\n",
    "beta = 100.0\n",
    "gamma = 0.95 \n",
    "\n",
    "exp_returns = expected_risky_returns\n",
    "\n",
    "Sigma_r = cov_mat_r\n",
    "\n",
    "# Generate the benchmark target portfolio by growing the initial portfolio value at rate eta\n",
    "\n",
    "target_return =  0.5 \n",
    "benchmark_portf = [ init_cash   * np.exp(dt * target_return)]\n",
    "\n",
    "rho = 0.4 \n",
    "\n",
    "for i in range(1,num_steps):\n",
    "    benchmark_portf.append(benchmark_portf[i-1]*np.exp(dt * target_return) )\n",
    "    \n",
    "print(benchmark_portf[0], benchmark_portf[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YT6003xKPQQm"
   },
   "source": [
    "### Simulate portfolio data\n",
    "\n",
    "Produce a list of trajectories, where each trajectory is a list made of state-action pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "So89W92zPLMq"
   },
   "outputs": [],
   "source": [
    "lambd = 0.001 \n",
    "omega = 1.0 \n",
    "beta = 1000.0 # fixed\n",
    "eta = 1.5 \n",
    "rho = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lCzgn95o8wUD"
   },
   "outputs": [],
   "source": [
    "reward_params=[lambd, omega, eta, rho]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "58NX65KOPQRD"
   },
   "outputs": [],
   "source": [
    "# Create a G-learner\n",
    "G_learner = G_learning_portfolio_opt(num_steps,\n",
    "                 reward_params,  \n",
    "                 beta,                \n",
    "                 benchmark_portf,\n",
    "                 gamma, \n",
    "                 num_risky_assets,\n",
    "                 riskfree_rate,\n",
    "                 expected_risky_returns, # array of shape num_steps x num_stocks\n",
    "                 Sigma_r,     # covariance matrix of returns of risky matrix                    \n",
    "                 x_vals_init, # array of initial values of len (num_stocks+1)\n",
    "                 use_for_WM = True) # use for wealth management tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mhy6IPKs7nRq",
    "outputId": "78fbc7d6-2869-48a9-f754-c9ed7c5127c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing G-learning, it may take a few seconds...\n"
     ]
    }
   ],
   "source": [
    "G_learner.reset_prior_policy()\n",
    "error_tol=1.e-8 \n",
    "max_iter_RL = 200\n",
    "G_learner.G_learning(error_tol, max_iter_RL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZJbI_k9mPQQo",
    "outputId": "45fc8ba1-fb6d-450b-8502-b96ffa56419a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done simulating trajectories in 484.593590 sec\n"
     ]
    }
   ],
   "source": [
    "num_sim = 1000\n",
    "trajs = []\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "t_0 = time.time()\n",
    "\n",
    "\n",
    "x_vals = [x_vals_init]\n",
    "returns_all = []\n",
    "for n in range(num_sim):\n",
    "    this_traj = []\n",
    "    x_t = x_vals_init[:]\n",
    "    returns_array = []\n",
    "    for t in range(0,num_steps):\n",
    "        \n",
    "        \n",
    "       \n",
    "        mu_t = G_learner.u_bar_prior[t,:] + G_learner.v_bar_prior[t,:].mv(torch.tensor(x_t))\n",
    "        u_t = np.random.multivariate_normal(mu_t.detach().numpy(), G_learner.Sigma_prior[t,:].detach().numpy())\n",
    "        # compute new values of x_t\n",
    "\n",
    "        x_next = x_t +u_t\n",
    "        # grow this with random return\n",
    "        \n",
    "        idiosync_vol =  0.05 # vol_market     \n",
    "        rand_norm = np.random.randn(num_risky_assets)\n",
    "        \n",
    "        # asset returns are simulated from a one-factor model\n",
    "        risky_asset_returns = (expected_risky_returns[t,:] + beta_vals * (returns_market[t] - mu_market * dt) \n",
    "                         + idiosync_vol * np.sqrt(1 - beta_vals**2) * np.sqrt(dt) * rand_norm)\n",
    "        \n",
    "        returns = np.hstack((riskfree_rate*dt, risky_asset_returns))\n",
    "        \n",
    "        x_next = (1+returns)*x_next\n",
    "        port_returns=(x_next.sum() -x_t.sum() -np.sum(u_t) - 0.015*np.abs(u_t).sum())/x_t.sum()\n",
    "        \n",
    "        this_traj.append((x_t, u_t))\n",
    "        \n",
    "        # rename\n",
    "        x_t = x_next\n",
    "        returns_array.append(port_returns) \n",
    "    # end the loop over time steps\n",
    "    trajs.append(this_traj)\n",
    "    returns_all.append(returns_array)\n",
    "\n",
    "print('Done simulating trajectories in %f sec'% (time.time() - t_0))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bpnem5H5vZ5H"
   },
   "source": [
    "### Calculate performance of G-learner (Diagnostics only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Z731JGirsdV"
   },
   "outputs": [],
   "source": [
    "returns_all_G=returns_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UIYUAIWvO04X",
    "outputId": "7e9f5055-b670-4a22-c6cf-d8082c03c9c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29292015743243366\n"
     ]
    }
   ],
   "source": [
    "SR_G=0\n",
    "for i in range(num_sim):\n",
    "  SR_G+=(np.mean(returns_all_G[i])-riskfree_rate*dt)/np.std(returns_all_G[i])\n",
    "\n",
    "SR_G/=num_sim\n",
    "print(SR_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TT9dIJP_t0K_"
   },
   "outputs": [],
   "source": [
    "r_G=np.array([0]*num_steps, dtype='float64')\n",
    "for n in range(num_steps):\n",
    "  for i in range(num_sim):\n",
    "    r_G[n]+=returns_all_G[i][n]\n",
    "  r_G[n]/=num_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "id": "KAdjSXoHui7R",
    "outputId": "9f5ea1f7-38b7-4aff-ef5a-e9ef9788651d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Sample Mean Returns')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(r_G, label='G-learning (' + str(np.round(SR_G,3)) + ')', color='red')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('time (quarters)')\n",
    "plt.ylabel('Sample Mean Returns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aTWkeFsDPQQu"
   },
   "outputs": [],
   "source": [
    "np.save('State_act_trajs.npy', trajs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "SNtCvuFMPQQ0",
    "outputId": "2f12bdef-6454-4811-9d4b-5ee128b9f596"
   },
   "outputs": [],
   "source": [
    "#trajs = np.load('State_act_trajs.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Wdu1t058d7a"
   },
   "source": [
    "## GIRL\n",
    "Implement the optimizer for finding the optimal G-learning parameters by MLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gH6Ho4m9yHvX"
   },
   "outputs": [],
   "source": [
    "def get_loss(trajs,\n",
    "             num_steps, \n",
    "             benchmark_portf, \n",
    "             gamma, \n",
    "             num_risky_assets, \n",
    "             riskfree_rate, \n",
    "             expected_risky_returns, \n",
    "             Sigma_r, \n",
    "             x_vals_init, \n",
    "             max_iter_RL, \n",
    "             reward_params,\n",
    "             beta, \n",
    "             num_trajs, \n",
    "             grad=False, \n",
    "             eps=1e-7):\n",
    "\n",
    "\n",
    "  data_xvals = torch.zeros(num_trajs,  num_steps, num_assets, dtype=torch.float64, requires_grad=False)\n",
    "  data_uvals = torch.zeros(num_trajs,  num_steps, num_assets, dtype=torch.float64, requires_grad=False)\n",
    "        \n",
    "  for n in range(num_trajs):\n",
    "        for t in range(num_steps):\n",
    "            data_xvals[n,t,:] = torch.tensor(trajs[n][t][0],dtype=torch.float64)\n",
    "            data_uvals[n,t,:] = torch.tensor(trajs[n][t][1],dtype=torch.float64)\n",
    "                \n",
    "                \n",
    "  # allocate memory for tensors that wil be used to compute the forward pass\n",
    "  realized_rewards = torch.zeros(num_trajs, num_steps, dtype=torch.float64, requires_grad=False)\n",
    "  realized_cum_rewards = torch.zeros(num_trajs, dtype=torch.float64, requires_grad=False)\n",
    "\n",
    "  realized_G_fun = torch.zeros(num_trajs, num_steps, dtype=torch.float64, requires_grad=False)\n",
    "  realized_F_fun  = torch.zeros(num_trajs,  num_steps, dtype=torch.float64, requires_grad=False)\n",
    "\n",
    "  realized_G_fun_cum = torch.zeros(num_trajs, dtype=torch.float64, requires_grad=False)  \n",
    "  realized_F_fun_cum = torch.zeros(num_trajs, dtype=torch.float64, requires_grad=False)  \n",
    "\n",
    "  reward_params_dict={}\n",
    "  loss_dict={}\n",
    "  loss_dict[-1]=np.array([0]*len(reward_params), dtype='float64') # perturb up\n",
    "  loss_dict[1]=np.array([0]*len(reward_params), dtype='float64') # perturb down\n",
    "  loss_grad = np.array([0]*len(reward_params), dtype='float64') \n",
    "\n",
    "  if grad: # compute gradient\n",
    "    for j in range(len(reward_params)):\n",
    "      for k in [-1,1]:\n",
    "            reward_params_dict[k]=reward_params\n",
    "            reward_params_dict[k][j]= reward_params_dict[k][j] + k*eps\n",
    "              \n",
    "            # 1. create a G-learner\n",
    "            G_learner = G_learning_portfolio_opt(num_steps,\n",
    "                                             reward_params_dict[k],\n",
    "                                             beta,\n",
    "                                             benchmark_portf,\n",
    "                                             gamma,\n",
    "                                             num_risky_assets,\n",
    "                                             riskfree_rate,\n",
    "                                             expected_risky_returns,\n",
    "                                             Sigma_r,\n",
    "                                             x_vals_init,\n",
    "                                             use_for_WM=True)\n",
    "        \n",
    "            G_learner.reset_prior_policy()\n",
    "        \n",
    "            # run the G-learning recursion to get parameters of G- and F-functions\n",
    "            G_learner.G_learning(error_tol, max_iter_RL)\n",
    "        \n",
    "            # compute the rewards and realized values of G- and F-functions from \n",
    "            # all trajectories\n",
    "            for n in range(num_trajs):\n",
    "              for t in range(num_steps):\n",
    "                \n",
    "                realized_rewards[n,t] = G_learner.compute_reward_on_traj(t,\n",
    "                                data_xvals[n,t,:], data_uvals[n,t,:])\n",
    "\n",
    "\n",
    "                realized_G_fun[n,t] = G_learner.compute_G_fun_on_traj(t,\n",
    "                                data_xvals[n,t,:], data_uvals[n,t,:])\n",
    "\n",
    "\n",
    "                realized_F_fun[n,t] = G_learner.compute_F_fun_on_traj(t,\n",
    "                                data_xvals[n,t,:])\n",
    "                \n",
    "              realized_cum_rewards[n] = realized_rewards[n,:].sum()\n",
    "              realized_G_fun_cum[n] = realized_G_fun[n,:].sum()\n",
    "              realized_F_fun_cum[n] = realized_F_fun[n,:].sum()\n",
    "          \n",
    "\n",
    "\n",
    "            \n",
    "            loss_dict[k][j] = - beta *(realized_G_fun_cum.sum() - realized_F_fun_cum.sum())\n",
    "      loss_grad[j]=(loss_dict[1][j]-loss_dict[-1][j])/(2.0*eps)\n",
    "\n",
    "  G_learner = G_learning_portfolio_opt(num_steps,\n",
    "                                      reward_params,\n",
    "                                      beta,\n",
    "                                      benchmark_portf,\n",
    "                                      gamma,\n",
    "                                      num_risky_assets,\n",
    "                                      riskfree_rate,\n",
    "                                      expected_risky_returns,\n",
    "                                      Sigma_r,\n",
    "                                      x_vals_init,\n",
    "                                      use_for_WM=True)\n",
    "        \n",
    "  G_learner.reset_prior_policy()\n",
    "        \n",
    "  G_learner.G_learning(error_tol, max_iter_RL)\n",
    "        \n",
    "  # compute the rewards and realized values of G- and F-functions from \n",
    "  # all trajectories\n",
    "  for n in range(num_trajs):\n",
    "      for t in range(num_steps):\n",
    "                \n",
    "                realized_rewards[n,t] = G_learner.compute_reward_on_traj(t,\n",
    "                                data_xvals[n,t,:], data_uvals[n,t,:])\n",
    "\n",
    "\n",
    "                realized_G_fun[n,t] = G_learner.compute_G_fun_on_traj(t,\n",
    "                                data_xvals[n,t,:], data_uvals[n,t,:])\n",
    "\n",
    "\n",
    "                realized_F_fun[n,t] = G_learner.compute_F_fun_on_traj(t,\n",
    "                                data_xvals[n,t,:])\n",
    "                \n",
    "      realized_cum_rewards[n] = realized_rewards[n,:].sum()\n",
    "      realized_G_fun_cum[n] = realized_G_fun[n,:].sum()\n",
    "      realized_F_fun_cum[n] = realized_F_fun[n,:].sum()\n",
    "          \n",
    "    \n",
    "\n",
    "  loss = - beta *(realized_G_fun_cum.sum() - realized_F_fun_cum.sum())   \n",
    "  if grad:\n",
    "    return loss, loss_grad\n",
    "  else:\n",
    "    return loss   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z2cYmHjoTZKM"
   },
   "outputs": [],
   "source": [
    "def fun(x, grad_=False, rescale=1.0, constraint=False):\n",
    "    y=x.copy()\n",
    "    y[0]/=sc[0]\n",
    "    y[1]/=sc[1]\n",
    "    y[2]/=sc[2]\n",
    "    y[3]/=sc[3]\n",
    "    with torch.no_grad():\n",
    "       if grad_==False:\n",
    "           if constraint:\n",
    "            y[0] = y[0]**2\n",
    "            y[1] = 1+y[1]**2\n",
    "            y[2] = 1+y[2]**2\n",
    "            y[3] = 1.0/(1.0+np.exp(-y[3]))\n",
    "\n",
    "           ret = rescale*get_loss(trajs,\n",
    "                                 num_steps, \n",
    "                                 benchmark_portf, \n",
    "                                 gamma, \n",
    "                                 num_risky_assets, \n",
    "                                 riskfree_rate, \n",
    "                                 expected_risky_returns, \n",
    "                                 Sigma_r, \n",
    "                                 x_vals_init, \n",
    "                                 max_iter_RL, \n",
    "                                 y, \n",
    "                                 beta,\n",
    "                                 len(trajs), \n",
    "                                 grad=grad_, \n",
    "                                 eps=1e-7).detach().numpy()\n",
    "\n",
    "           print(ret)\n",
    "           return ret\n",
    "       else:\n",
    "            f, df = get_loss(trajs,\n",
    "                                 num_steps, \n",
    "                                 benchmark_portf, \n",
    "                                 gamma, \n",
    "                                 num_risky_assets, \n",
    "                                 riskfree_rate, \n",
    "                                 expected_risky_returns, \n",
    "                                 Sigma_r, \n",
    "                                 x_vals_init, \n",
    "                                 max_iter_RL, \n",
    "                                 y, \n",
    "                                 beta,\n",
    "                                 len(trajs), \n",
    "                                 grad=grad_, \n",
    "                                 eps=1e-7)\n",
    "            return f.detach().numpy()*rescale, df*rescale/sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JQXvu1HqPQRc"
   },
   "source": [
    "### GIRL (G-learning IRL)\n",
    "Two different optimizers are used to demonstrate the implementation of GIRL. The first approach is gradient free and the second approach uses the gradient of the loss function. \n",
    "#### Gradient free\n",
    "Initialize the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "58IbcwyOTm9S"
   },
   "outputs": [],
   "source": [
    "lambd_0 = 0.002\n",
    "omega_0 = 1.1\n",
    "eta_0 = 1.3 \n",
    "beta_0 = beta \n",
    "rho_0 = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qK1h_-r1UCN8"
   },
   "outputs": [],
   "source": [
    "sc=np.array([1,1,1,1]) # optional re-scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O3A81-tocRuz"
   },
   "outputs": [],
   "source": [
    "x0=np.array([lambd_0, omega_0, eta_0, rho_0])\n",
    "x0=[np.sqrt(x0[0]), np.sqrt(x0[1]-1), np.sqrt(x0[2]-1), np.log(x0[3]/(1-x0[3]))]\n",
    "x0*=sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "XdgT7EKxZVDf",
    "outputId": "77a4f480-7d1b-4c78-f021-75f36e51e47a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing G-learning, it may take a few seconds...\n",
      "0.11664879271454365\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.11664879271454365"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test evaluate the loss function\n",
    "fun(x0, False, 1e-9, constraint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "colab_type": "code",
    "id": "NR9zW91ndDVR",
    "outputId": "aef06036-ee0e-4ab4-cc7f-4ea85627a0a5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing G-learning, it may take a few seconds...\n",
      "0.11664879271454365\n",
      "Doing G-learning, it may take a few seconds...\n",
      "0.23021764926766605\n",
      "Doing G-learning, it may take a few seconds...\n",
      "0.11083604772491008\n",
      "Doing G-learning, it may take a few seconds...\n",
      "0.13367159804593026\n",
      "Doing G-learning, it may take a few seconds...\n",
      "0.11646813196787983\n",
      "Doing G-learning, it may take a few seconds...\n",
      "0.04378962202791125\n",
      "Doing G-learning, it may take a few seconds...\n",
      "0.004345664879538119\n",
      "Doing G-learning, it may take a few seconds...\n",
      "0.03276377141602337\n",
      "Doing G-learning, it may take a few seconds...\n",
      "0.014636591549500824\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-0.0015728140432909132\n",
      "Doing G-learning, it may take a few seconds...\n",
      "0.015722762478023768\n",
      "Doing G-learning, it may take a few seconds...\n",
      "0.01238071551538259\n",
      "Doing G-learning, it may take a few seconds...\n",
      "0.016892217851005496\n",
      "Doing G-learning, it may take a few seconds...\n",
      "0.0010367978194653989\n",
      "Doing G-learning, it may take a few seconds...\n",
      "0.025350016575329006\n",
      "Doing G-learning, it may take a few seconds...\n",
      "0.0010293354473635554\n",
      "Doing G-learning, it may take a few seconds...\n",
      "0.029615932315833867\n",
      "Doing G-learning, it may take a few seconds...\n",
      "0.0002115229714885354\n",
      "Doing G-learning, it may take a few seconds...\n",
      "0.005621533483184875\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-0.0007532481117248535\n",
      "Doing G-learning, it may take a few seconds...\n",
      "0.005570083419390023\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-0.0015115474610105158\n",
      "Doing G-learning, it may take a few seconds...\n",
      "0.0014685308244973422\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-0.001424356220059097\n",
      "Doing G-learning, it may take a few seconds...\n",
      "0.005035305204227567\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-0.0016529708211347463\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.001653\n",
      "         Iterations: 13\n",
      "         Function evaluations: 26\n"
     ]
    }
   ],
   "source": [
    "# Optimize with the Nelder-Mead method\n",
    "res = minimize(fun, x0, method='Nelder-Mead', args=(False, 1e-9, True), options={'disp': True, 'maxiter':50}, tol=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "id": "HRAO1V1_dtfd",
    "outputId": "2059a172-03ae-499d-e929-0e158baa8afd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " final_simplex: (array([[3.89069770e-02, 3.39793264e-01, 5.56662365e-01, 2.69487703e-04],\n",
       "       [3.96902066e-02, 3.47356437e-01, 5.50290007e-01, 1.17187500e-04],\n",
       "       [3.88212890e-02, 3.40820247e-01, 5.61390457e-01, 2.62927651e-04],\n",
       "       [3.96161637e-02, 3.39277023e-01, 5.55933989e-01, 2.61240080e-04],\n",
       "       [3.96670735e-02, 3.34824592e-01, 5.62564058e-01, 2.21180916e-04]]), array([-0.00165297, -0.00157281, -0.00151155, -0.00142436, -0.00075325]))\n",
       "           fun: -0.0016529708211347463\n",
       "       message: 'Optimization terminated successfully.'\n",
       "          nfev: 26\n",
       "           nit: 13\n",
       "        status: 0\n",
       "       success: True\n",
       "             x: array([3.89069770e-02, 3.39793264e-01, 5.56662365e-01, 2.69487703e-04])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bw2lEM2Tn9tz"
   },
   "outputs": [],
   "source": [
    "# Print parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "D7zhJ8K5ePvA",
    "outputId": "c3025c91-c657-4f33-ed2a-665045ecc375"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0015137528590772958,\n",
       " 1.1154594619557496,\n",
       " 1.3098729891569825,\n",
       " 0.500067371925397)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.x[0]**2/sc[0], (1+res.x[1]**2)/sc[1], (1+res.x[2]**2)/sc[2], 1.0/(1+np.exp(-res.x[3]))/sc[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1TpYxXmq89QD"
   },
   "source": [
    "#### GIRL (Gradient based)\n",
    "Now separately perform GIRL using a gradient based optimizer. This has the advantage of being more accurate but can be less stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VPZgTZ789R_H"
   },
   "outputs": [],
   "source": [
    "# initialize the parameters\n",
    "lambd_0 = 0.002\n",
    "omega_0 = 1.1\n",
    "eta_0 = 1.3 \n",
    "beta_0 = beta \n",
    "rho_0 = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z9nO_7hHBGai"
   },
   "outputs": [],
   "source": [
    "x0=np.array([lambd_0, omega_0, eta_0, rho_0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MLHLm-uU9XSu"
   },
   "outputs": [],
   "source": [
    "# rescaling\n",
    "sc=np.array([1000,0.1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "yEO1I-P2DpLp",
    "outputId": "17c8508d-1071-40fe-9735-2a6d37ae705c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing G-learning, it may take a few seconds...\n",
      "0.9065929408371448\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9065929408371448"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test evaluation of the loss function\n",
    "x_ask=np.array([lambd, omega, eta, rho])*sc\n",
    "fun(x_ask, False, 1e-6, constraint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WCZdikslULik"
   },
   "outputs": [],
   "source": [
    "# choose bounds for parameters\n",
    "bnds=((0.0001*sc[0], 0.0025*sc[0]), (0.01*sc[1], 1.5*sc[1]), (1.001*sc[2],2*sc[2]), (0.01*sc[3],1*sc[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "yScEwb3xUcSc",
    "outputId": "597e90db-1c56-4f6a-81d5-3ea9e9a26d5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing G-learning, it may take a few seconds...\n",
      "2145.675664575592\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2145.675630645961\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2145.675769964847\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2145.675658493792\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2145.675658792118\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-1.0547312405820849e+20\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-1.4886212272562866e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-1.1998281989772216e+18\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-8.12151991759787e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-4.136571440528917e+18\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-1.0547312405820849e+20\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-1.4886212272562866e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-1.1998281989772216e+18\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-8.12151991759787e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-4.136571440528917e+18\n",
      "Doing G-learning, it may take a few seconds...\n",
      "14153693861.342896\n",
      "Doing G-learning, it may take a few seconds...\n",
      "14155364832.31701\n",
      "Doing G-learning, it may take a few seconds...\n",
      "14154412477.076124\n",
      "Doing G-learning, it may take a few seconds...\n",
      "14153334294.540266\n",
      "Doing G-learning, it may take a few seconds...\n",
      "14154156128.250126\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-3.6381049770364883e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.2201240994806856e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-1.520021988387332e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-5.006319047390536e+18\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-1.4079377930726703e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-2.9642067337186103e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-2.1640712654281044e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-2.1272886626926334e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.3340201158420916e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-2.8111494137887044e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "5.329023532713635e+25\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-1.6815609899274878e+22\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-1.6138051248193733e+25\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-1.468267483078682e+25\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-6.194877221018463e+26\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-5.7043767214731485e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-1.3090562263731865e+20\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-4.719965486277644e+20\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2.066946748625855e+20\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2.1764770953090646e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.480166898480898e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-8.443089558106705e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.5923851176483365e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2.1217248657549466e+20\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2.7215587629845306e+20\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2.5381087145957937e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-1.5120706735688274e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-2.102377832329696e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-9.55193602046016e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-3.202328929100024e+18\n",
      "Doing G-learning, it may take a few seconds...\n",
      "8.262489011697579e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-7.327298532952026e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "4.0720688311410524e+20\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.0405691689533925e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-2.1595049301729344e+18\n",
      "Doing G-learning, it may take a few seconds...\n",
      "8.929734758095114e+18\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-5.672649704930754e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-1.1735005152190367e+20\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-6.708173957571614e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-3.7291866517984676e+18\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-4.733142154695925e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-1.3666218135384749e+20\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2.071211589638551e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-2.5949659329910022e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-8.5964279514626e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-1.615749502229945e+20\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-3.1648852035297874e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-1.949791282111248e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-7.574621618655656e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-6.053804544950234e+18\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-4.408360175160386e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2.156954752215655e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2.4542263778276483e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-5.216734187193925e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "7.388501527844313e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.1695764636287725e+20\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-7.49878849525164e+18\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-9.287658157543377e+18\n",
      "Doing G-learning, it may take a few seconds...\n",
      "4.871242774806195e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-1.4991422169342472e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-1.0257529576704604e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-1.4734352045018866e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-1.0174283665725458e+20\n",
      "Doing G-learning, it may take a few seconds...\n",
      "9.38384277455992e+18\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-1.48218852483652e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2.670638084276659e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-2.4153210955467874e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "8.881910623837721e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-5.8748688898174796e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "6.0422959072622895e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-1.4182709501226447e+18\n",
      "Doing G-learning, it may take a few seconds...\n",
      "7.004187323266668e+20\n",
      "Doing G-learning, it may take a few seconds...\n",
      "9.140051186502499e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "5.027671696929677e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-1.2024958183306858e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "3.2922658315751293e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "9.822776599696707e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-6.21316971635907e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "4.0174134617038134e+20\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-1.0590708582507159e+18\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-1.4137477457840395e+20\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.113395831278484e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-1.2719440002667928e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-1.924326758083613e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-4.1090139678444446e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-7.2746927167403205e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-8.961680319189089e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "9.75547077147137e+18\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-1.2618660632654556e+20\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-3.1407657535534408e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-9.67090743281383e+18\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-3.989612109276497e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.1945416980010009e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-2.0557234457789448e+18\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-1.144976828852451e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "736.7485656814202\n",
      "Doing G-learning, it may take a few seconds...\n",
      "736.7484991229153\n",
      "Doing G-learning, it may take a few seconds...\n",
      "736.7444734079288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing G-learning, it may take a few seconds...\n",
      "736.7485657553873\n",
      "Doing G-learning, it may take a few seconds...\n",
      "736.7485804147208\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-2.2484371172594966e+17\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-6.106524409873029e+17\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-1.375176107538695e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.9441898864486646e+18\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2.0247337569864945e+18\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-1.0637697997698339e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-2.969456304379006e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.8166929485556122e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-3.006226915718074e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-8.118555433927329e+18\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-3.6136172160966866e+18\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-3.934747088215282e+18\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2.3119845018584523e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.7903819253150177e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-7.349066710294163e+18\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-2.8965108458446673e+26\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2.8889178217267753e+27\n",
      "Doing G-learning, it may take a few seconds...\n",
      "4.48375599952782e+26\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-1.0497712051598949e+33\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-6.631199020322029e+31\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-7.1688879712570835e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-2.7894991067561705e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "3.335009016065776e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-2.836224623444623e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-2.0967997974550118e+18\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2.1003861025844783e+21\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-5.6288714455644135e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "8.824921045886268e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.416972928805758e+20\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2.1123876641959405e+20\n",
      "Doing G-learning, it may take a few seconds...\n",
      "3.817046220413925e+21\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-4.547301214359005e+21\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2.1061457002279432e+21\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-4.2637349208912193e+21\n",
      "Doing G-learning, it may take a few seconds...\n",
      "4.935386279139089e+22\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.6763070331773134e+22\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-1.3289246374771145e+22\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-5.90105164542536e+23\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.994907628352379e+22\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-4.267792901470419e+21\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-2.8965108458446673e+26\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2.8889178217267753e+27\n",
      "Doing G-learning, it may take a few seconds...\n",
      "4.48375599952782e+26\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-1.0497712051598949e+33\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-6.631199020322029e+31\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-4.4827540030651466e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-3.0691583447301923e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-1.5492955600370237e+18\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-3.8302066603099455e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.1682478045503817e+20\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-5.062227772028798e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "4.213347761859637e+21\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-2.71930681035375e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2.1226536496195702e+20\n",
      "Doing G-learning, it may take a few seconds...\n",
      "3.297338096701991e+19\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.7146101758076899e+21\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.1766800126861625e+21\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2.075976907165066e+22\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.2502512627963384e+22\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-1.9289709640750743e+21\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.633799329773109e+23\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2.599927643646674e+24\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-1.527170284056927e+23\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-3.683455550894004e+23\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-1.3431049085435804e+23\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.4426291068164603e+28\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-5.73030327115958e+29\n",
      "Doing G-learning, it may take a few seconds...\n",
      "7.950155595916898e+28\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-1.3889645370910047e+28\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-3.981836496467005e+28\n",
      "Doing G-learning, it may take a few seconds...\n",
      "3.6882442812429386e+27\n",
      "Doing G-learning, it may take a few seconds...\n",
      "3.7004025123479805e+29\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.1841572170711307e+30\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-2.1234764275346087e+29\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-7.826027929578456e+34\n",
      "Doing G-learning, it may take a few seconds...\n",
      "5.968097167784172e+27\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-1.2473902643615418e+26\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2.054521618595939e+33\n",
      "Doing G-learning, it may take a few seconds...\n",
      "3.8597899670243635e+27\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.1846827378251095e+30\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2.1042942938562004e+28\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2.273305188497234e+28\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-5.718276718784577e+26\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-2.1864912217717928e+30\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-3.502500470561473e+26\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-1.8128595178837e+26\n",
      "Doing G-learning, it may take a few seconds...\n",
      "4.93358002113409e+25\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2.3990514373573166e+24\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.7624103467484532e+27\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-3.150720622075618e+29\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2.0434302065283763e+26\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-2.7755060975399954e+27\n",
      "Doing G-learning, it may take a few seconds...\n",
      "6.43379322633681e+27\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-1.6411286693706664e+29\n",
      "Doing G-learning, it may take a few seconds...\n",
      "9.678768686441057e+29\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-4.644796400896825e+29\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-1.9253412074350267e+28\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2.703852913734367e+30\n",
      "Doing G-learning, it may take a few seconds...\n",
      "3.575687395809855e+28\n",
      "Doing G-learning, it may take a few seconds...\n",
      "3.673021419252605e+27\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.0721523849719251e+27\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-3.876654826594043e+32\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.595498372598817e+31\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.1666604665447488e+27\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2.7367236107091954e+25\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-1.569822999537083e+28\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-5.522032128437313e+27\n",
      "Doing G-learning, it may take a few seconds...\n",
      "3.645164330393189e+36\n",
      "Doing G-learning, it may take a few seconds...\n",
      "3.6091259845467765e+25\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.954010385957907e+30\n",
      "Doing G-learning, it may take a few seconds...\n",
      "3.516467491862801e+28\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-5.991561161522267e+25\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.8187282377847817e+30\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2.041978429101416e+33\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-4.916822686726051e+26\n",
      "Doing G-learning, it may take a few seconds...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.024822616950716e+31\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-2.5587995678982457e+32\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.252860978299805e+27\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.222592450829973e+31\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-8.43282932742564e+28\n",
      "Doing G-learning, it may take a few seconds...\n",
      "3.8252125360051136e+33\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-1.701444894658711e+25\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-3.3508071406584635e+29\n",
      "Doing G-learning, it may take a few seconds...\n",
      "6.719021193629729e+34\n",
      "Doing G-learning, it may take a few seconds...\n",
      "9.837343967255996e+29\n",
      "Doing G-learning, it may take a few seconds...\n",
      "7.273895195596448e+31\n",
      "Doing G-learning, it may take a few seconds...\n",
      "3.3849103504135534e+30\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.346606856846544e+30\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-9.442554039728885e+26\n",
      "Doing G-learning, it may take a few seconds...\n",
      "2.5233489896682433e+26\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-3.024822616950716e+31\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-2.5587995678982457e+32\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.252860978299805e+27\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.222592450829973e+31\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-8.43282932742564e+28\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-3.024822616950716e+31\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-2.5587995678982457e+32\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.252860978299805e+27\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.222592450829973e+31\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-8.43282932742564e+28\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-3.024822616950716e+31\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-2.5587995678982457e+32\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.252860978299805e+27\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.222592450829973e+31\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-8.43282932742564e+28\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-3.024822616950716e+31\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-2.5587995678982457e+32\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.252860978299805e+27\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.222592450829973e+31\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-8.43282932742564e+28\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-3.024822616950716e+31\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-2.5587995678982457e+32\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.252860978299805e+27\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.222592450829973e+31\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-8.43282932742564e+28\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-3.024822616950716e+31\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-2.5587995678982457e+32\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.252860978299805e+27\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.222592450829973e+31\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-8.43282932742564e+28\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-3.024822616950716e+31\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-2.5587995678982457e+32\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.252860978299805e+27\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.222592450829973e+31\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-8.43282932742564e+28\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-3.024822616950716e+31\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-2.5587995678982457e+32\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.252860978299805e+27\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.222592450829973e+31\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-8.43282932742564e+28\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-3.024822616950716e+31\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-2.5587995678982457e+32\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.252860978299805e+27\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.222592450829973e+31\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-8.43282932742564e+28\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-3.024822616950716e+31\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-2.5587995678982457e+32\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.252860978299805e+27\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.222592450829973e+31\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-8.43282932742564e+28\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-3.024822616950716e+31\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-2.5587995678982457e+32\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.252860978299805e+27\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.222592450829973e+31\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-8.43282932742564e+28\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-3.024822616950716e+31\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-2.5587995678982457e+32\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.252860978299805e+27\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.222592450829973e+31\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-8.43282932742564e+28\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-3.024822616950716e+31\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-2.5587995678982457e+32\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.252860978299805e+27\n",
      "Doing G-learning, it may take a few seconds...\n",
      "1.222592450829973e+31\n",
      "Doing G-learning, it may take a few seconds...\n",
      "-8.43282932742564e+28\n"
     ]
    }
   ],
   "source": [
    "# L-BFGS-B for gradient solver with bounds\n",
    "res = minimize(fun, x0, method='L-BFGS-B', bounds=bnds, args=(False, 1e-6, False), options={'disp': True, 'maxiter':50}, tol=0.0000001) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "colab_type": "code",
    "id": "_6APDqOx-Tqx",
    "outputId": "41710a5d-050d-46fc-b135-659b64d74da2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: -3.024822616950716e+31\n",
       " hess_inv: <4x4 LbfgsInvHessProduct with dtype=float64>\n",
       "      jac: array([-2.25631731e+40,  3.02494790e+39,  4.24741507e+39,  3.01638979e+39])\n",
       "  message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
       "     nfev: 315\n",
       "      nit: 4\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([2.46850418e+00, 1.00000000e-03, 1.98688987e+00, 9.87007974e-01])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9TcGgfihDQG1",
    "outputId": "16c1069e-f90d-443f-f897-87c7d9ea3892"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0024685 , 0.01      , 1.98688987, 0.98700797])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print results. Note that these may differ from the actual G-learner parameters depending on the optimizer. \n",
    "# The optimizer will attempt to find the closest set of parameters.\n",
    "res.x/sc"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "Wealth_Management_GIRL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
