{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mfrdixon/ML_Finance_Codes/blob/master/Wealth_Management_GIRL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vc1tLT83PQMd"
   },
   "outputs": [],
   "source": [
    "# ML_in_Finance-GIRL-wealth-management\n",
    "# Author: Igor Halperin and Matthew Dixon\n",
    "# Version: 1.0 (12.8.2019)\n",
    "# License: MIT\n",
    "# Email: ighalp@gmail.com\n",
    "# Notes: tested on Mac OS X with Python 3.6 and PyTorch 1.3.0\n",
    "# Citation: Please cite the following reference if this notebook is used for research purposes:\n",
    "# Dixon M.F., I. Halperin and P. Bilokon, Machine Learning in Finance: From Theory to Practice, Springer Graduate textbook Series, 2020. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uXdeHDmEPQM6"
   },
   "source": [
    "## G-learing and GIRL for wealth optimization\n",
    "\n",
    "This notebook demonstrates the application of G-learning and GIRL for optimization of a defined contribution retirement plan. The notebook extends the G-learning notebook in Chapter 10 with an example of applying GIRL to infer the parameters of the G-learner used to generate the trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vOoE16hWPQNC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "from scipy.optimize import minimize\n",
    "import torch\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "M-6MadDnPQNO",
    "outputId": "176e0a88-271e-4be2-9d69-1f13e1520534"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PHyhUTpJ4nl1",
    "outputId": "18b1c78b-d6c6-4453-9755-65390364ab53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: KMP_DUPLICATE_LIB_OK=TRUE\n"
     ]
    }
   ],
   "source": [
    "%env KMP_DUPLICATE_LIB_OK=TRUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S0yKeMU9PQNa"
   },
   "outputs": [],
   "source": [
    "# set the device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qLivr9yuPQON"
   },
   "outputs": [],
   "source": [
    "# Define the G-learning portfolio optimization class\n",
    "class G_learning_portfolio_opt:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 num_steps,\n",
    "                 params,\n",
    "                 beta,\n",
    "                 benchmark_portf,\n",
    "                 gamma, \n",
    "                 num_risky_assets,\n",
    "                 riskfree_rate,\n",
    "                 exp_returns, # array of shape num_steps x num_stocks\n",
    "                 Sigma_r,     # covariance matrix of returns of risky assets\n",
    "                 init_x_vals, # array of initial asset position values (num_risky_assets + 1)\n",
    "                 use_for_WM = True): # use for wealth management tasks\n",
    "\n",
    "                \n",
    "        self.num_steps = num_steps\n",
    "        self.num_assets = num_risky_assets + 1 # exp_returns.shape[1]\n",
    "        \n",
    "        self.lambd = torch.tensor(params[0], requires_grad=False, dtype=torch.float64)\n",
    "        self.Omega_mat = params[1] * torch.eye(self.num_assets,dtype=torch.float64)\n",
    "        #self.Omega_mat = torch.tensor(Omega_mat,requires_grad=False, dtype=torch.float64)\n",
    "        self.eta = torch.tensor(params[2], requires_grad=False, dtype=torch.float64)\n",
    "        self.rho = torch.tensor(params[3], requires_grad=False, dtype=torch.float64)\n",
    "        self.beta = torch.tensor(beta, requires_grad=False, dtype=torch.float64)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.use_for_WM = use_for_WM\n",
    "        \n",
    "        self.num_risky_assets = num_risky_assets\n",
    "        self.r_f = riskfree_rate\n",
    "        \n",
    "        \n",
    "        assert exp_returns.shape[0] == self.num_steps\n",
    "        assert Sigma_r.shape[0] == Sigma_r.shape[1]\n",
    "        assert Sigma_r.shape[0] == num_risky_assets # self.num_assets\n",
    "        \n",
    "        self.Sigma_r_np = Sigma_r # array of shape num_stocks x num_stocks\n",
    "        \n",
    "        self.reg_mat = 1e-3*torch.eye(self.num_assets, dtype=torch.float64)\n",
    "        \n",
    "        # arrays of returns for all assets including the risk-free asset\n",
    "        # array of shape num_steps x (num_stocks + 1) \n",
    "        self.exp_returns_np = np.hstack((self.r_f * np.ones(self.num_steps).reshape((-1,1)), exp_returns))\n",
    "                                      \n",
    "        # make block-matrix Sigma_r_tilde with Sigma_r_tilde[0,0] = 0, and equity correlation matrix inside\n",
    "        self.Sigma_r_tilde_np = np.zeros((self.num_assets, self.num_assets))\n",
    "        self.Sigma_r_tilde_np[1:,1:] = self.Sigma_r_np\n",
    "            \n",
    "        # make Torch tensors  \n",
    "        self.exp_returns = torch.tensor(self.exp_returns_np,requires_grad=False, dtype=torch.float64)\n",
    "        self.Sigma_r = torch.tensor(Sigma_r,requires_grad=False, dtype=torch.float64)\n",
    "        self.Sigma_r_tilde = torch.tensor(self.Sigma_r_tilde_np,requires_grad=False, dtype=torch.float64)\n",
    "        \n",
    "        self.benchmark_portf = torch.tensor(benchmark_portf, requires_grad=False, dtype=torch.float64)\n",
    "        \n",
    "        # asset holding values for all times. Initialize with initial values, \n",
    "        # values for the future times will be expected values \n",
    "        self.x_vals_np = np.zeros((self.num_steps, self.num_assets))\n",
    "        self.x_vals_np[0,:] = init_x_vals \n",
    "        \n",
    "        # Torch tensor\n",
    "        self.x_vals = torch.tensor(self.x_vals_np)\n",
    "                \n",
    "        # allocate memory for coefficients of R-, F- and G-functions        \n",
    "        self.F_xx = torch.zeros(self.num_steps, self.num_assets, self.num_assets, dtype=torch.float64,\n",
    "                                requires_grad=True)\n",
    "        self.F_x = torch.zeros(self.num_steps, self.num_assets, dtype=torch.float64,\n",
    "                               requires_grad=True)\n",
    "        self.F_0 = torch.zeros(self.num_steps,dtype=torch.float64,requires_grad=True)\n",
    "        \n",
    "        self.Q_xx = torch.zeros(self.num_steps, self.num_assets, self.num_assets,dtype=torch.float64,\n",
    "                                requires_grad=True)\n",
    "        self.Q_uu = torch.zeros(self.num_steps, self.num_assets, self.num_assets,dtype=torch.float64,\n",
    "                                requires_grad=True)\n",
    "        self.Q_ux = torch.zeros(self.num_steps, self.num_assets, self.num_assets,dtype=torch.float64,\n",
    "                                requires_grad=True)\n",
    "        self.Q_x = torch.zeros(self.num_steps, self.num_assets,dtype=torch.float64,requires_grad=True)\n",
    "        self.Q_u = torch.zeros(self.num_steps, self.num_assets,dtype=torch.float64,requires_grad=True)\n",
    "        self.Q_0 = torch.zeros(self.num_steps,dtype=torch.float64,requires_grad=True)\n",
    "        \n",
    "        self.R_xx = torch.zeros(self.num_steps, self.num_assets, self.num_assets,dtype=torch.float64,\n",
    "                                requires_grad=True)\n",
    "        self.R_uu = torch.zeros(self.num_steps, self.num_assets, self.num_assets,dtype=torch.float64,\n",
    "                                requires_grad=True)\n",
    "        self.R_ux = torch.zeros(self.num_steps, self.num_assets, self.num_assets,dtype=torch.float64,\n",
    "                                requires_grad=True)\n",
    "        self.R_x = torch.zeros(self.num_steps, self.num_assets,dtype=torch.float64,requires_grad=True)\n",
    "        self.R_u = torch.zeros(self.num_steps, self.num_assets,dtype=torch.float64,requires_grad=True)\n",
    "        self.R_0 = torch.zeros(self.num_steps,dtype=torch.float64,requires_grad=True)\n",
    "\n",
    "        \n",
    "        self.reset_prior_policy()\n",
    "        \n",
    "        # the list of adjustable model parameters:\n",
    "        self.model_params = [self.lambd, self.beta, self.Omega_mat, self.eta]  \n",
    "#                              self.exp_returns, self.Sigma_r_tilde,self.Sigma_prior_inv, self.u_bar_prior]\n",
    "        \n",
    "        \n",
    "        # expected cash installment for all steps\n",
    "        self.expected_c_t = torch.zeros(self.num_steps,dtype=torch.float64)\n",
    "        \n",
    "        # realized values of the target portfolio\n",
    "        self.realized_target_portf = np.zeros(self.num_steps,dtype=np.float64)\n",
    "        \n",
    "        # expected portfolio values for all times\n",
    "        self.expected_portf_val = torch.zeros(self.num_steps,dtype=torch.float64)\n",
    "        \n",
    "        # the first value is the sum of initial position values\n",
    "        self.expected_portf_val[0] = self.x_vals[0,:].sum()\n",
    "\n",
    "    def reset_prior_policy(self):\n",
    "        # initialize time-dependent parameters of prior policy \n",
    "        self.u_bar_prior = torch.zeros(self.num_steps,self.num_assets,requires_grad=False,\n",
    "                                       dtype=torch.float64)\n",
    "        self.v_bar_prior =  torch.zeros(self.num_steps, self.num_assets, self.num_assets,requires_grad=False,\n",
    "                                        dtype=torch.float64)\n",
    "        self.Sigma_prior =  torch.zeros(self.num_steps, self.num_assets, self.num_assets,requires_grad=False,\n",
    "                                        dtype=torch.float64)\n",
    "        self.Sigma_prior_inv = torch.zeros(self.num_steps, self.num_assets, self.num_assets,requires_grad=False,\n",
    "                                        dtype=torch.float64)\n",
    "        \n",
    "        # make each time elements of v_bar_prior and Sigma_prior proportional to the unit matrix\n",
    "        for t in range(self.num_steps):\n",
    "            self.v_bar_prior[t,:,:] = 0.1 * torch.eye(self.num_assets).clone()\n",
    "            self.Sigma_prior[t,:,:] = 0.1 * torch.eye(self.num_assets).clone()\n",
    "            self.Sigma_prior_inv[t,:,:] = 10.0 * torch.eye(self.num_assets).clone() # np.linalg.inv(self.Sigma_prior[t,:,:])\n",
    "    \n",
    "    def reward_fun(self, t, x_vals, u_vals, exp_rets, lambd, Sigma_hat):\n",
    "        \"\"\"\n",
    "        The reward function \n",
    "        \"\"\"\n",
    "        x_plus = x_vals + u_vals\n",
    "        \n",
    "        p_hat = self.rho.clone() * self.benchmark_portf[t] + (1-self.rho.clone())*self.eta.clone()*x_vals.sum()\n",
    "        \n",
    "        aux_1 = - self.lambd.clone() * p_hat**2         \n",
    "        aux_2 = - u_vals.sum()   \n",
    "        aux_3 = 2*self.lambd.clone() * p_hat * x_plus.dot(torch.ones(num_assets) + exp_rets)\n",
    "        aux_4 = - self.lambd.clone() * x_plus.mm(Sigma_hat.mv(x_plus))\n",
    "        aux_5 = - u_vals.mm(self.Omega_mat.clone().mv(u_vals))\n",
    "        \n",
    "        return aux_1 + aux_2 + aux_3 + aux_4 + aux_5  \n",
    "    \n",
    "    def compute_reward_fun(self):\n",
    "        \"\"\"\n",
    "        Compute coefficients R_xx, R_ux, etc. for all steps\n",
    "        \"\"\"\n",
    "        for t in range(0, self.num_steps):\n",
    "            \n",
    "            one_plus_exp_ret = torch.ones(self.num_assets,dtype=torch.float64) + self.exp_returns[t,:]\n",
    "            benchmark_portf = self.benchmark_portf[t]\n",
    "            Sigma_hat = self.Sigma_r_tilde + torch.ger(one_plus_exp_ret, one_plus_exp_ret)\n",
    "            \n",
    "            one_plus_exp_ret_by_one = torch.ger(one_plus_exp_ret,torch.ones(self.num_assets,dtype=torch.float64))\n",
    "            one_plus_exp_ret_by_one_T = one_plus_exp_ret_by_one.t()     \n",
    "            one_one_T_mat = torch.ones(self.num_assets,self.num_assets)\n",
    "            \n",
    "            self.R_xx[t,:,:] = (-self.lambd.clone()*(self.eta.clone()**2)*(self.rho.clone()**2)*one_one_T_mat\n",
    "                                 + 2*self.lambd.clone()*self.eta.clone()*self.rho.clone()*one_plus_exp_ret_by_one\n",
    "                                 - self.lambd.clone()*Sigma_hat)\n",
    "            \n",
    "            self.R_ux[t,:,:] = (2*self.lambd.clone()*self.eta.clone()*self.rho.clone()*one_plus_exp_ret_by_one\n",
    "                                 - 2*self.lambd.clone()*Sigma_hat)\n",
    "            \n",
    "            self.R_uu[t,:,:] = - self.lambd.clone() * Sigma_hat - self.Omega_mat.clone()\n",
    "            \n",
    "            self.R_x[t,:] =  (-2*self.lambd.clone()*self.eta.clone()*self.rho.clone()*(1-self.rho.clone())*benchmark_portf *\n",
    "                                 torch.ones(self.num_assets,dtype=torch.float64)\n",
    "                                 + 2*self.lambd.clone()*(1-self.rho.clone())*benchmark_portf * one_plus_exp_ret)\n",
    "            \n",
    "            self.R_u[t,:] = (2*self.lambd.clone()*(1-self.rho.clone())*benchmark_portf * one_plus_exp_ret\n",
    "                             - torch.ones(self.num_assets,dtype=torch.float64))\n",
    "            \n",
    "            self.R_0[t] = - self.lambd.clone()*((1-self.rho.clone())**2) * (benchmark_portf**2)\n",
    "                \n",
    "         \n",
    "    def project_cash_injections(self):\n",
    "        \"\"\"\n",
    "        Compute the expected values of future asset positions, and the expected cash injection for future steps,\n",
    "        as well as realized values of the target portfolio\n",
    "        \"\"\"\n",
    "           \n",
    "        # this assumes that the policy is trained\n",
    "        for t in range(1, self.num_steps):  # the initial value is fixed \n",
    "            \n",
    "            # increment the previous x_t\n",
    "            \n",
    "            delta_x_t = self.u_bar_prior[t,:] + self.v_bar_prior[t,:,:].mv(self.x_vals[t-1,:])\n",
    "            self.x_vals[t,:] = self.x_vals[t-1,:] + delta_x_t\n",
    "            \n",
    "            # grow using the expected return\n",
    "            self.x_vals[t,:] = (torch.ones(self.num_assets)+ self.exp_returns[t,:])*self.x_vals[t,:]\n",
    "            \n",
    "            # compute c_t\n",
    "            self.expected_c_t[t] = delta_x_t.sum().data # detach().numpy()\n",
    "            \n",
    "            # expected portfolio value for this step\n",
    "            self.expected_portf_val[t] = self.x_vals[t,:].sum().data # .detach().numpy()\n",
    "    \n",
    "            \n",
    "                                                                                      \n",
    "    def set_terminal_conditions(self):\n",
    "        \"\"\"\n",
    "        set the terminal condition for the F-function\n",
    "        \"\"\"\n",
    "        \n",
    "        # the auxiliary quantity to perform matrix calculations\n",
    "        one_plus_exp_ret = torch.ones(self.num_assets,dtype=torch.float64) + self.exp_returns[-1,:]\n",
    "        \n",
    "        \n",
    "        # Compute the reward function for all steps (only the last step is needed for this functions, while \n",
    "        # values for other time steps will be used in other functions)\n",
    "        self.compute_reward_fun()\n",
    "        \n",
    "        if self.use_for_WM:\n",
    "\n",
    "            Sigma_hat = self.Sigma_r_tilde + torch.ger(one_plus_exp_ret, one_plus_exp_ret)\n",
    "            Sigma_hat_inv = torch.inverse(Sigma_hat + self.reg_mat)\n",
    "            \n",
    "            Sigma_tilde = Sigma_hat + (1/self.lambd)*self.Omega_mat.clone()\n",
    "            #Sigma_tilde_inv = torch.pinverse(Sigma_tilde)\n",
    "            Sigma_tilde_inv = torch.inverse(Sigma_tilde + self.reg_mat)\n",
    "            \n",
    "            Sigma_hat_sigma_tilde = Sigma_hat.mm(Sigma_tilde)\n",
    "            Sigma_tilde_inv_sig_hat = Sigma_tilde_inv.mm(Sigma_hat)\n",
    "            Sigma_tilde_sigma_hat = Sigma_tilde.mm(Sigma_hat)\n",
    "            \n",
    "            Sigma_hat_Sigma_tilde_inv = Sigma_hat.mm(Sigma_tilde_inv)\n",
    "            Sigma_3_plus_omega = self.lambd*Sigma_tilde_inv.mm(Sigma_hat_Sigma_tilde_inv) + self.Omega_mat.clone()    \n",
    "                             \n",
    "            one_plus_exp_ret_by_one = torch.ger(one_plus_exp_ret,torch.ones(self.num_assets,dtype=torch.float64))\n",
    "            one_plus_exp_ret_by_one_T = one_plus_exp_ret_by_one.t()     \n",
    "            one_one_T_mat = torch.ones(self.num_assets,self.num_assets)\n",
    "            \n",
    "            Sigma_tilde_inv_t_R_ux = Sigma_tilde_inv.t().mm(self.R_ux[-1,:,:].clone())\n",
    "            Sigma_tilde_inv_t_R_uu = Sigma_tilde_inv.t().mm(self.R_uu[-1,:,:].clone())\n",
    "            Sigma_tilde_inv_t_R_u = Sigma_tilde_inv.t().mv(self.R_u[-1,:].clone())\n",
    "            \n",
    "            Sigma_tilde_inv_R_u = Sigma_tilde_inv.mv(self.R_u[-1,:].clone())\n",
    "            Sigma_tilde_inv_R_ux = Sigma_tilde_inv.mm(self.R_ux[-1,:,:].clone())\n",
    "            Sigma_tilde_inv_t_R_uu = Sigma_tilde_inv.mm(self.R_uu[-1,:,:].clone())\n",
    "            \n",
    "            # though the action at the last step is deterministic, we can feed \n",
    "            # parameters of the prior with these values                     \n",
    "              \n",
    "            self.u_bar_prior[-1,:]   = (1/(2 * self.lambd.clone()))* Sigma_tilde_inv.clone().mv(self.R_u[-1,:].clone())\n",
    "            self.v_bar_prior[-1,:,:] = (1/(2 * self.lambd.clone()))* Sigma_tilde_inv.clone().mm(self.R_ux[-1,:,:].clone())    \n",
    "                \n",
    "            # First compute the coefficients of the reward function at the last step\n",
    "\n",
    "            \n",
    "            # the coefficients of F-function for the last step\n",
    "            \n",
    "            # F_xx                 \n",
    "            self.F_xx[-1,:,:] = (self.R_xx[-1,:,:].clone()\n",
    "                                 + (1/(2*self.lambd.clone()))* self.R_ux[-1,:,:].clone().t().mm(Sigma_tilde_inv_t_R_ux)\n",
    "                                 + (1/(4*self.lambd.clone()**2))* self.R_ux[-1,:,:].clone().t().mm(\n",
    "                                      Sigma_tilde_inv_t_R_uu.clone().mm(Sigma_tilde_inv.clone().mm(self.R_ux[-1,:,:].clone())))\n",
    "                                )\n",
    "            \n",
    "            # F_x                    \n",
    "            self.F_x[-1,:] = (self.R_x[-1,:].clone()\n",
    "                                 + (1/(self.lambd.clone()))* self.R_ux[-1,:,:].clone().t().mv(Sigma_tilde_inv_t_R_u.clone())\n",
    "                                 + (1/(2*self.lambd.clone()**2))* self.R_ux[-1,:,:].clone().t().mv(\n",
    "                                      Sigma_tilde_inv_t_R_uu.clone().mv(Sigma_tilde_inv_R_u.clone()))\n",
    "                            )\n",
    "            \n",
    "            \n",
    "        \n",
    "            # F_0   \n",
    "            self.F_0[-1] = (self.R_0[-1].clone() \n",
    "                            +  (1/(2*self.lambd.clone()))* self.R_u[-1,:].clone().dot(Sigma_tilde_inv_R_u.clone())\n",
    "                            + (1/(4*self.lambd.clone()**2))* self.R_u[-1,:].clone().dot(\n",
    "                                Sigma_tilde_inv_t_R_uu.clone().mv(Sigma_tilde_inv_R_u.clone()))\n",
    "                           )\n",
    "            \n",
    "            # for the Q-function at the last step:\n",
    "            self.Q_xx[-1,:,:] = self.R_xx[-1,:,:].clone()\n",
    "            self.Q_ux[-1,:,:] = self.R_ux[-1,:,:].clone()\n",
    "            self.Q_uu[-1,:,:] = self.R_uu[-1,:,:].clone()\n",
    "            self.Q_u[-1,:] = self.R_u[-1,:].clone()\n",
    "            self.Q_x[-1,:] = self.R_x[-1,:].clone()\n",
    "            self.Q_0[-1] = self.R_0[-1].clone()\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "                \n",
    "    def G_learning(self, err_tol, max_iter):\n",
    "        \"\"\"\n",
    "        find the optimal policy for the time dependent policy\n",
    "        \n",
    "        \"\"\"   \n",
    "        print('Doing G-learning, it may take a few seconds...')\n",
    "        \n",
    "        # set terminal conditions\n",
    "        self.set_terminal_conditions()\n",
    "        \n",
    "        # allocate iteration numbers for all steps\n",
    "        self.iter_counts = np.zeros(self.num_steps)\n",
    "        \n",
    "        # iterate over time steps backward\n",
    "        for t in range(self.num_steps-2,-1,-1):\n",
    "            self.step_G_learning(t, err_tol, max_iter)\n",
    "            \n",
    "    def step_G_learning(self, t, err_tol, max_iter):\n",
    "        \"\"\"\n",
    "        Perform one step of backward iteration for G-learning self-consistent equations\n",
    "        This should start from step t = num_steps - 2 (i.e. from a step that is before the last one)\n",
    "        \"\"\"\n",
    "            \n",
    "        # make matrix Sigma_hat_t        \n",
    "        one_plus_exp_ret = torch.ones(self.num_assets,dtype=torch.float64) + self.exp_returns[t,:]\n",
    "        Sigma_hat_t = self.Sigma_r_tilde + torch.ger(one_plus_exp_ret, one_plus_exp_ret)\n",
    "        \n",
    "        # matrix A_t = diag(1 + r_bar_t)\n",
    "        A_t = torch.diag(torch.ones(self.num_assets,dtype=torch.float64) + self.exp_returns[t,:])\n",
    "                    \n",
    "        # update parameters of Q_function using next-step F-function values\n",
    "        self.update_Q_params(t, A_t,Sigma_hat_t)\n",
    "             \n",
    "        # iterate between policy evaluation and policy improvement  \n",
    "        while self.iter_counts[t] < max_iter:\n",
    "                \n",
    "            curr_u_bar_prior = self.u_bar_prior[t,:].clone()  \n",
    "            curr_v_bar_prior = self.v_bar_prior[t,:,:].clone()     \n",
    "                \n",
    "            # compute parameters of F-function for this step from parameters of Q-function\n",
    "            self.update_F_params(t) \n",
    "              \n",
    "            # Policy iteration step: update parameters of the prior policy distribution\n",
    "            # with given Q- and F-function parameters\n",
    "            self.update_policy_params(t)    \n",
    "            \n",
    "            # difference between the current value of u_bar_prior and the previous one\n",
    "            err_u_bar = torch.sum((curr_u_bar_prior - self.u_bar_prior[t,:])**2)\n",
    "            \n",
    "            # divide by num_assets in err_v_bar to get both errors on a comparable scale\n",
    "            err_v_bar = (1/self.num_assets)*torch.sum((curr_v_bar_prior - self.v_bar_prior[t,:,:])**2)\n",
    "            \n",
    "            # choose the difference from the previous iteration as the maximum of the two errors\n",
    "            tol = torch.max(err_u_bar, err_v_bar)  # tol = 0.5*(err_u_bar + err_v_bar)\n",
    "            \n",
    "            self.iter_counts[t] += 1\n",
    "            # Repeat the calculation of Q- and F-values\n",
    "            if tol <= err_tol:\n",
    "                break\n",
    "                \n",
    "            \n",
    "    def update_Q_params(self,t, A_t,Sigma_hat_t):\n",
    "        \"\"\"\n",
    "        update the current (time-t) parameters of Q-function from (t+1)-parameters of F-function\n",
    "        \"\"\" \n",
    "                \n",
    "        ones = torch.ones(self.num_assets,dtype=torch.float64)    \n",
    "        one_plus_exp_ret = torch.ones(self.num_assets,dtype=torch.float64) + self.exp_returns[t,:]\n",
    "    \n",
    "        self.Q_xx[t,:,:] = (self.R_xx[t,:,:].clone() \n",
    "                            + self.gamma *( (A_t.clone().mm(self.F_xx[t+1,:,:].clone())).mm(A_t.clone())  \n",
    "                                           + self.Sigma_r_tilde.clone() * self.F_xx[t+1,:,:].clone() ) )\n",
    "\n",
    "\n",
    "        self.Q_ux[t,:,:] = (self.R_ux[t,:,:].clone() \n",
    "                            + 2 * self.gamma *( (A_t.clone().mm(self.F_xx[t+1,:,:].clone())).mm(A_t.clone())  \n",
    "                                           + self.Sigma_r_tilde.clone() * self.F_xx[t+1,:,:].clone() ) \n",
    "                           )\n",
    "    \n",
    "        self.Q_uu[t,:,:] = (self.R_uu[t,:,:].clone()  \n",
    "                            + self.gamma *( (A_t.clone().mm(self.F_xx[t+1,:,:].clone())).mm(A_t.clone())  \n",
    "                                           + self.Sigma_r_tilde.clone() * self.F_xx[t+1,:,:].clone() )\n",
    "                            - self.Omega_mat.clone()\n",
    "                           )\n",
    "\n",
    "\n",
    "        self.Q_x[t,:] = self.R_x[t,:].clone() + self.gamma * A_t.clone().mv(self.F_x[t+1,:].clone()) \n",
    "        self.Q_u[t,:] = self.R_u[t,:].clone() + self.gamma * A_t.clone().mv(self.F_x[t+1,:].clone())\n",
    "        self.Q_0[t]   = self.R_0[t].clone() + self.gamma * self.F_0[t+1].clone()\n",
    "\n",
    "\n",
    "        \n",
    "    def update_F_params(self,t):\n",
    "        \"\"\"\n",
    "        update the current (time-t) parameters of F-function from t-parameters of G-function\n",
    "        This is a policy evaluation step: it uses the current estimations of the mean parameters of the policy\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # produce auxiliary parameters U_t, W_t, Sigma_tilde_t\n",
    "        U_t = (self.beta.clone() * self.Q_ux[t,:,:].clone() \n",
    "               + self.Sigma_prior_inv[t,:,:].clone().mm(self.v_bar_prior[t,:,:].clone()))\n",
    "        W_t = (self.beta.clone() * self.Q_u[t,:].clone() \n",
    "               +  self.Sigma_prior_inv[t,:,:].clone().mv(self.u_bar_prior[t,:]).clone())\n",
    "        Sigma_p_bar =  self.Sigma_prior_inv[t,:,:].clone() - 2 * self.beta.clone() * self.Q_uu[t,:,:].clone()\n",
    "        Sigma_p_bar_inv = torch.inverse(Sigma_p_bar + self.reg_mat)\n",
    "        \n",
    "        # update parameters of F-function\n",
    "        self.F_xx[t,:,:] = self.Q_xx[t,:,:].clone() + (1/(2*self.beta.clone()))*(U_t.t().mm(Sigma_p_bar_inv.clone().mm(U_t))\n",
    "                                    - self.v_bar_prior[t,:,:].clone().t().mm(\n",
    "                                        self.Sigma_prior_inv[t,:,:].clone().mm(self.v_bar_prior[t,:,:].clone())))\n",
    "        \n",
    "        \n",
    "        self.F_x[t,:] = self.Q_x[t,:].clone() + (1/self.beta.clone())*(U_t.mv(Sigma_p_bar_inv.clone().mv(W_t))\n",
    "                                    - self.v_bar_prior[t,:,:].clone().mv(\n",
    "                                        self.Sigma_prior_inv[t,:,:].clone().mv(self.u_bar_prior[t,:].clone())))\n",
    "        \n",
    "        \n",
    "        self.F_0[t] = self.Q_0[t].clone() + ( (1/(2*self.beta.clone()))*(W_t.dot(Sigma_p_bar_inv.clone().mv(W_t))\n",
    "                                    - self.u_bar_prior[t,:].clone().dot(\n",
    "                                        self.Sigma_prior_inv[t,:,:].clone().mv(self.u_bar_prior[t,:].clone())))\n",
    "                                    - (1/(2*self.beta.clone())) * (torch.log(torch.det(self.Sigma_prior[t,:,:].clone()+\n",
    "                                                                              self.reg_mat))\n",
    "                                                       - torch.log(torch.det(Sigma_p_bar_inv.clone() + self.reg_mat))) )\n",
    "        \n",
    "        \n",
    "        \n",
    "    def update_policy_params(self,t):\n",
    "        \"\"\"\n",
    "        update parameters of the Gaussian policy using current coefficients of the F- and G-functions\n",
    "        \"\"\"\n",
    "        \n",
    "        new_Sigma_prior_inv = self.Sigma_prior_inv[t,:,:].clone() - 2 * self.beta.clone() * self.Q_uu[t,:,:].clone()\n",
    "        \n",
    "        Sigma_prior_new = torch.inverse(new_Sigma_prior_inv + self.reg_mat)\n",
    "        \n",
    "        # update parameters using the previous value of Sigma_prior_inv\n",
    "        self.u_bar_prior[t,:] = Sigma_prior_new.mv(self.Sigma_prior_inv[t,:,:].clone().mv(self.u_bar_prior[t,:].clone())\n",
    "                                              + self.beta.clone() * self.Q_u[t,:].clone())\n",
    "        \n",
    "        \n",
    "        self.v_bar_prior[t,:,:] = Sigma_prior_new.clone().mm(self.Sigma_prior_inv[t,:,:].clone().mm(self.v_bar_prior[t,:,:].clone())\n",
    "                                              + self.beta.clone() * self.Q_ux[t,:,:].clone())\n",
    "        \n",
    "        # and then assign the new inverse covariance for the prior for the next iteration\n",
    "        self.Sigma_prior[t,:,:] = Sigma_prior_new.clone()\n",
    "        self.Sigma_prior_inv[t,:,:] = new_Sigma_prior_inv.clone()\n",
    "        \n",
    "        # also assign the same values for the previous time step\n",
    "        if t > 0:\n",
    "            self.Sigma_prior[t-1,:,:] = self.Sigma_prior[t,:,:].clone()\n",
    "            self.u_bar_prior[t-1,:] = self.u_bar_prior[t,:].clone()\n",
    "            self.v_bar_prior[t-1,:,:] = self.v_bar_prior[t,:,:].clone()\n",
    "            \n",
    "    def trajs_to_torch_tensors(self,trajs):\n",
    "        \"\"\"\n",
    "        Convert data from a list of lists into Torch tensors\n",
    "        \"\"\"\n",
    "        num_trajs = len(trajs)\n",
    "        \n",
    "        self.data_xvals = torch.zeros(num_trajs,self.num_steps,self.num_assets,dtype=torch.float64)\n",
    "        self.data_uvals = torch.zeros(num_trajs,self.num_steps,self.num_assets,dtype=torch.float64)\n",
    "            \n",
    "        for n in range(num_trajs):\n",
    "            for t in range(self.num_steps):\n",
    "                self.data_xvals[n,t,:] = torch.tensor(trajs[n][t][0],dtype=torch.float64).clone()\n",
    "                self.data_uvals[n,t,:] = torch.tensor(trajs[n][t][1],dtype=torch.float64).clone()\n",
    "                \n",
    "    def compute_reward_on_traj(self,\n",
    "                              t,\n",
    "                              x_t, u_t):\n",
    "        \"\"\"\n",
    "        Given time t and corresponding values of vectors x_t, u_t, compute the total reward for this step\n",
    "        \"\"\"\n",
    "        \n",
    "        aux_xx = x_t.dot(self.R_xx[t,:,:].clone().mv(x_t))\n",
    "        aux_ux = u_t.dot(self.R_ux[t,:,:].clone().mv(x_t))\n",
    "        aux_uu = u_t.dot(self.R_uu[t,:,:].clone().mv(u_t))\n",
    "        aux_x = x_t.dot(self.R_x[t,:].clone())\n",
    "        aux_u = u_t.dot(self.R_u[t,:].clone())\n",
    "        aux_0 = self.R_0[t].clone()\n",
    "        \n",
    "        return aux_xx + aux_ux + aux_uu + aux_x + aux_u + aux_0\n",
    "    \n",
    "    def compute_G_fun_on_traj(self,\n",
    "                              t,\n",
    "                              x_t, u_t):\n",
    "        \"\"\"\n",
    "        Given time t and corresponding values of vectors x_t, u_t, compute the total reward for this step\n",
    "        \"\"\"\n",
    "        \n",
    "        aux_xx = x_t.dot(self.Q_xx[t,:,:].clone().mv(x_t))\n",
    "        aux_ux = u_t.dot(self.Q_ux[t,:,:].clone().mv(x_t))\n",
    "        aux_uu = u_t.dot(self.Q_uu[t,:,:].clone().mv(u_t))\n",
    "        aux_x = x_t.dot(self.Q_x[t,:].clone())\n",
    "        aux_u = u_t.dot(self.Q_u[t,:].clone())\n",
    "        aux_0 = self.Q_0[t].clone()\n",
    "        \n",
    "        return aux_xx + aux_ux + aux_uu + aux_x + aux_u + aux_0\n",
    "    \n",
    "    def compute_F_fun_on_traj(self,\n",
    "                              t,\n",
    "                              x_t):\n",
    "        \"\"\"\n",
    "        Given time t and corresponding values of vectors x_t, u_t, compute the total reward for this step\n",
    "        \"\"\"\n",
    "        \n",
    "        aux_xx = x_t.dot(self.F_xx[t,:,:].clone().mv(x_t))\n",
    "        aux_x = x_t.dot(self.F_x[t,:].clone())\n",
    "        aux_0 = self.F_0[t].clone()\n",
    "        \n",
    "        return aux_xx + aux_x + aux_0\n",
    "\n",
    "                 \n",
    "    def MaxEntIRL(self,\n",
    "                  trajs,\n",
    "                  learning_rate,\n",
    "                  err_tol, max_iter):\n",
    "        \n",
    "        \"\"\"\n",
    "        Estimate parameters of the reward function using MaxEnt IRL.\n",
    "        Inputs:\n",
    "        \n",
    "        trajs - a list of trajectories. Each trajectory is a list of state-action pairs, stored as a tuple.\n",
    "                We assume each trajectory has the same length\n",
    "        \"\"\"\n",
    "        \n",
    "        # omega is a tunable parameter that determines the cost matrix self.Omega_mat\n",
    "        omega_init = 15.0\n",
    "        self.omega = torch.tensor(omega_init, requires_grad=True, dtype=torch.float64)\n",
    "        \n",
    "        # also set beta to be small \n",
    "        beta_init = 50 \n",
    "        self.beta = torch.tensor(beta_init, requires_grad=True, dtype=torch.float64)\n",
    "        \n",
    "        reward_params =  [self.lambd, self.eta, self.rho, self.omega, self.beta]\n",
    "        \n",
    "        print(\"Omega mat...\")\n",
    "        self.Omega_mat = self.omega * torch.eye(self.num_assets,dtype=torch.float64)\n",
    "        print(\"g learning...\")\n",
    "        self.reset_prior_policy()\n",
    "        self.G_learning(err_tol, max_iter)\n",
    "        print(\"intialize optimizer...\")\n",
    "        optimizer = optim.Adam(reward_params, lr=learning_rate)\n",
    "        print(\"zero grad...\")\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        num_trajs = len(trajs)\n",
    "        print(\"trajs_to_torch_tensors...\")\n",
    "        # fill in Torch tensors for the trajectory data\n",
    "        self.trajs_to_torch_tensors(trajs)\n",
    "        print(\"constructing zero tensors...\")   \n",
    "        self.realized_rewards = torch.zeros(num_trajs,self.num_steps,dtype=torch.float64,requires_grad=True)\n",
    "        self.realized_cum_rewards = torch.zeros(num_trajs, dtype=torch.float64, requires_grad=True)\n",
    "        print(\"constructing zero tensors...\")  \n",
    "        self.realized_G_fun = torch.zeros(num_trajs,self.num_steps,dtype=torch.float64, requires_grad=True)\n",
    "        self.realized_F_fun = torch.zeros(num_trajs,self.num_steps,dtype=torch.float64, requires_grad=True)\n",
    "        print(\"constructing zero tensors...\")  \n",
    "        self.realized_G_fun_cum = torch.zeros(num_trajs,dtype=torch.float64, requires_grad=True)\n",
    "        self.realized_F_fun_cum = torch.zeros(num_trajs,dtype=torch.float64, requires_grad=True)\n",
    "        print(\"done...\")  \n",
    "        \n",
    "        num_iter_IRL = 3\n",
    "        \n",
    "        for i in range(num_iter_IRL):\n",
    "            \n",
    "            print('GIRL iteration = ', i)\n",
    "       \n",
    "            self.Omega_mat = self.omega * torch.eye(self.num_assets,dtype=torch.float64)\n",
    "    \n",
    "            for n in range(101):\n",
    "                if n%100==0:\n",
    "                    print(n)\n",
    "                for t in range(self.num_steps):\n",
    "                    \n",
    "                    \n",
    "                    # compute rewards obtained at each step for each trajectory\n",
    "                    # given the model paramaters\n",
    "        \n",
    "                    self.realized_rewards[n,t] = self.compute_reward_on_traj(t,\n",
    "                                                                self.data_xvals[n,t,:],\n",
    "                                                                self.data_uvals[n,t,:])\n",
    "                                                                \n",
    "            \n",
    "                    # compute the log-likelihood by looping over trajectories\n",
    "                    self.realized_G_fun[n,t] = self.compute_G_fun_on_traj(t,\n",
    "                                                                self.data_xvals[n,t,:],\n",
    "                                                                self.data_uvals[n,t,:])\n",
    "                \n",
    "                \n",
    "                    self.realized_F_fun[n,t] = self.compute_F_fun_on_traj(t,\n",
    "                                                                self.data_xvals[n,t,:])\n",
    "                \n",
    "\n",
    "                self.realized_cum_rewards[n] = self.realized_rewards[n,:].sum().clone()\n",
    "                self.realized_G_fun_cum[n] = self.realized_G_fun[n,:].sum().clone()\n",
    "                self.realized_F_fun_cum[n] = self.realized_F_fun[n,:].sum().clone()\n",
    "            \n",
    "            # the negative log-likelihood will not include terms ~ Sigma_p as we do not optimize over its value\n",
    "            loss = - self.beta.clone()*(self.realized_G_fun_cum.sum().clone() - self.realized_F_fun_cum.sum().clone())\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            loss.backward() \n",
    "        \n",
    "            optimizer.step()\n",
    "        \n",
    "            print('Iteration = ', i)\n",
    "            print('Loss = ', loss.detach().numpy())\n",
    "        \n",
    "           \n",
    "        print('Done optimizing reward parameters')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N_8XeDlpPQOV"
   },
   "source": [
    "## Simulate portfolio data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gy2u0fkyPQOX"
   },
   "source": [
    "### Simulate the market factor under a lognormal distribution with a fixed drift and vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F25fZ-s9PQOb"
   },
   "outputs": [],
   "source": [
    "mu_market = 0.05\n",
    "vol_market = 0.25\n",
    "init_market_val = 100.0\n",
    "\n",
    "r_rf = 0.02  # risk-free rate - the first asset will be cash\n",
    "\n",
    "num_steps = 10\n",
    "dt = 0.25 # quarterly time steps\n",
    "\n",
    "num_risky_assets = 99 \n",
    "\n",
    "returns_market = np.zeros(num_steps)\n",
    "market_vals = np.zeros(num_steps)\n",
    "market_vals[0] = 100.0  # initial value\n",
    "\n",
    "\n",
    "        \n",
    "for t in range(1,num_steps):\n",
    "\n",
    "        rand_norm = np.random.randn()\n",
    "        \n",
    "        # use log-returns of market as 'returns_market'\n",
    "        returns_market[t] = mu_market * dt + vol_market * np.sqrt(dt) * rand_norm\n",
    "        \n",
    "        market_vals[t] = market_vals[t-1] * np.exp((mu_market - 0.5*vol_market**2)*dt + \n",
    "                                                         vol_market*np.sqrt(dt)*rand_norm)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sUUBRbwbPQO1"
   },
   "source": [
    "### Simulate market betas and idiosyncratic alphas within pre-defined ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "qraR7fRVPQO4",
    "outputId": "dfb260b8-fa17-4a70-a12d-86a4c2964e3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.38888481 0.23879842 0.08635828 0.56004803 0.06598472 0.75617181\n",
      " 0.34627799 0.74072757 0.79658177 0.24476291]\n",
      "[0.01616581 0.00395208 0.06559046 0.08333434 0.07694703 0.06277679\n",
      " 0.10888204 0.10132533 0.12329485 0.10018176]\n"
     ]
    }
   ],
   "source": [
    "beta_min = 0.05\n",
    "beta_max = 0.85\n",
    "beta_vals = np.random.uniform(low=beta_min, high=beta_max, size=num_risky_assets)\n",
    "\n",
    "alpha_min = - 0.05\n",
    "alpha_max = 0.15\n",
    "alpha_vals = np.random.uniform(low=alpha_min, high=alpha_max, size=num_risky_assets)\n",
    "\n",
    "print(beta_vals[0:10])\n",
    "print(alpha_vals[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DNFzG5dgPQPB"
   },
   "source": [
    "### Simulate time-dependent expected returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a36GRMAlPQPD"
   },
   "outputs": [],
   "source": [
    "# Time-independent expected returns would be equal to alpha + beta * expected_market_return \n",
    "# Make them time-dependent (and correlated with actual returns) as alpha + beta * oracle_market_returns\n",
    "\n",
    "oracle_coeff = 0.2\n",
    "mu_vec = mu_market * np.ones(num_steps)\n",
    "oracle_market_returns = mu_vec * dt + oracle_coeff*(returns_market - mu_vec)\n",
    "\n",
    "expected_risky_returns = np.zeros((num_steps, num_risky_assets))\n",
    "\n",
    "for t in range(num_steps):\n",
    "    expected_risky_returns[t,:] = alpha_vals * dt + beta_vals * oracle_market_returns[t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DDN_R3znPQPK"
   },
   "source": [
    "### Initial values of all assets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YneTj6IBPQPM"
   },
   "outputs": [],
   "source": [
    "val_min = 20.0\n",
    "val_max = 120.0\n",
    "\n",
    "init_risky_asset_vals = np.random.uniform(low=val_min, high=val_max, size=num_risky_assets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nqd7WIxbPQPS"
   },
   "source": [
    "### Simulate realized returns and asset prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tSL2TU1IPQPW"
   },
   "outputs": [],
   "source": [
    "# Generate realized returns and realized asset values by simulating from a one-factor model \n",
    "# with time-dependent expected returns\n",
    "\n",
    "risky_asset_returns = np.zeros((num_steps, num_risky_assets))\n",
    "risky_asset_vals = np.zeros((num_steps, num_risky_assets))\n",
    "\n",
    "idiosync_vol =  0.05 # vol_market \n",
    "\n",
    "for t in range(num_steps):\n",
    "    \n",
    "    rand_norm = np.random.randn(num_risky_assets)\n",
    "        \n",
    "    # asset returns are simulated from a one-factor model\n",
    "    risky_asset_returns[t,:] = (expected_risky_returns[t,:] + beta_vals * (returns_market[t] - mu_market * dt) \n",
    "                         + idiosync_vol * np.sqrt(1 - beta_vals**2) * np.sqrt(dt) * rand_norm)\n",
    "        \n",
    "    # asset values\n",
    "    if t == 0:\n",
    "        risky_asset_vals[t,:] = init_risky_asset_vals\n",
    "    else:\n",
    "        risky_asset_vals[t] = risky_asset_vals[t-1] * (1 + risky_asset_returns[t,:])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "YvHvV7B6PQPh",
    "outputId": "3421ed28-a65f-4362-d166-990d22dbf1fd"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare the realized and expected returns\n",
    "# Note that they should appear correlated.\n",
    "\n",
    "# pick a random asset ID to show\n",
    "asset_idx =  4 \n",
    "\n",
    "plt.plot(expected_risky_returns[:,asset_idx],label='expected_return')\n",
    "plt.plot(risky_asset_returns[:,asset_idx],label='realized_return',color='r')\n",
    "plt.legend()\n",
    "plt.xlabel('Time Steps')\n",
    "plt.title('Realized returns vs expected returns')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WqAdjcoFPQPp"
   },
   "source": [
    "### Compute the empirical correlation matrix using realized returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sl5JQ3_8PQPs",
    "outputId": "5f93d784-2e83-4a2c-bf2e-23bd42eb5fb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99, 99)\n"
     ]
    }
   ],
   "source": [
    "cov_mat_r = np.cov(risky_asset_returns.T) \n",
    "\n",
    "print(cov_mat_r.shape)\n",
    "\n",
    "D,v = np.linalg.eigh(cov_mat_r)\n",
    "\n",
    "eigenvals = D[::-1]  # put them in a descended order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "H-HzoeHOPQP0",
    "outputId": "96c11580-037d-430d-fe77-5e54635ea519"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.18438721e-01, 7.82418568e-03, 6.04329219e-03, 5.85191397e-03,\n",
       "       4.77084015e-03, 4.44418054e-03, 4.25298938e-03, 3.96754172e-03,\n",
       "       3.53635223e-03, 4.63010281e-17])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eigenvalues: the largest eigenvalue is the market factor \n",
    "eigenvals[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "hFC61Ci94nnd",
    "outputId": "fc1d8cde-9ada-4e39-f6e6-3fe3b55559e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 33.9064,   9.3252,   9.4684,  ..., -12.5213,   9.5298,   8.1695],\n",
       "        [  9.3252,  39.1112,   9.4735,  ..., -15.4381,   9.9311,  -7.9070],\n",
       "        [  9.4684,   9.4735,   9.2873,  ...,  -4.1432,   2.4905,  -0.9692],\n",
       "        ...,\n",
       "        [-12.5213, -15.4381,  -4.1432,  ...,  29.9866,   5.9177,   0.3197],\n",
       "        [  9.5298,   9.9311,   2.4905,  ...,   5.9177,  15.1844,   2.0105],\n",
       "        [  8.1695,  -7.9070,  -0.9692,  ...,   0.3197,   2.0105,  10.4434]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov_mat_torch = torch.tensor(cov_mat_r)\n",
    "\n",
    "torch.pinverse(cov_mat_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ohSC6oDePQP5"
   },
   "source": [
    "### Add a riskless bond as one more asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F7nwaM28PQP9"
   },
   "outputs": [],
   "source": [
    "num_assets = num_risky_assets + 1\n",
    "\n",
    "bond_val = 100.0\n",
    "\n",
    "# add the bond to initial assets\n",
    "init_asset_vals = np.hstack((np.array([bond_val]),\n",
    "                            init_risky_asset_vals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hZQgn_CTPQQE"
   },
   "source": [
    "### Make the initial portfolio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PyMLBotBPQQH"
   },
   "outputs": [],
   "source": [
    "# consider here two choices: equal or equally-weighted \n",
    "\n",
    "init_port_choice =  'equal' \n",
    "\n",
    "init_cash = 1000.0\n",
    "init_total_asset = np.sum(init_asset_vals)\n",
    "\n",
    "x_vals_init = np.zeros(num_assets)\n",
    "\n",
    "if init_port_choice == 'equal': \n",
    "    # hold equal amounts of cash in each asset\n",
    "    amount_per_asset = init_cash/num_assets\n",
    "    x_vals_init = amount_per_asset * np.ones(num_assets)\n",
    "\n",
    "elif init_port_choice == 'equally_weighted':\n",
    "    amount_per_asset = init_cash/init_total_asset\n",
    "    x_vals_init = amount_per_asset * init_asset_vals\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zkj8GdPDPQQP"
   },
   "source": [
    "### Make the target portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LyyKCaPGPQQS",
    "outputId": "bda0d8d5-b848-4c2b-c36b-3f2ac1a1bf78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1100.0 1541.5835692311712\n"
     ]
    }
   ],
   "source": [
    "# make a target portfolio term structure by defining it as the initial portfolio growing at some fixed and high rate\n",
    "\n",
    "target_portfolio = [init_cash]\n",
    "\n",
    "target_return = 0.15 \n",
    "coeff_target = 1.1 \n",
    "\n",
    "for i in range(1,num_steps):\n",
    "    target_portfolio.append(target_portfolio[i-1]*np.exp(dt * target_return) )\n",
    "    \n",
    "target_portfolio = coeff_target*np.array(target_portfolio)    \n",
    "print(target_portfolio[0], target_portfolio[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2UMA8UwlPQQc"
   },
   "source": [
    "### Define model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5G_Y87m2PQQe",
    "outputId": "0cf27618-1f16-4676-9741-4e2c3ce8d466"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1133.1484530668263 3490.3429574618413\n"
     ]
    }
   ],
   "source": [
    "riskfree_rate = 0.02\n",
    "fee_bond = 0.05 \n",
    "fee_stock = 0.05 \n",
    "\n",
    "all_fees = np.zeros(num_risky_assets + 1)\n",
    "all_fees[0] = fee_bond\n",
    "all_fees[1:] = fee_stock\n",
    "Omega_mat = np.diag(all_fees)\n",
    "\n",
    "\n",
    "# model parameters\n",
    "\n",
    "lambd = 0.001 \n",
    "Omega_mat = 15.5 * np.diag(all_fees) \n",
    "eta = 1.5 \n",
    "\n",
    "beta = 100.0\n",
    "gamma = 0.95 \n",
    "\n",
    "exp_returns = expected_risky_returns\n",
    "\n",
    "Sigma_r = cov_mat_r\n",
    "\n",
    "# Generate the benchmark target portfolio by growing the initial portfolio value at rate eta\n",
    "\n",
    "target_return =  0.5 \n",
    "benchmark_portf = [ init_cash   * np.exp(dt * target_return)]\n",
    "\n",
    "rho = 0.4 \n",
    "\n",
    "for i in range(1,num_steps):\n",
    "    benchmark_portf.append(benchmark_portf[i-1]*np.exp(dt * target_return) )\n",
    "    \n",
    "print(benchmark_portf[0], benchmark_portf[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YT6003xKPQQm"
   },
   "source": [
    "### Simulate portfolio data\n",
    "\n",
    "Produce a list of trajectories, where each trajectory is a list made of state-action pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "So89W92zPLMq"
   },
   "outputs": [],
   "source": [
    "lambd = 0.001 \n",
    "omega = 1.0 \n",
    "beta = 1000.0 # fixed\n",
    "eta = 1.5 \n",
    "rho = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lCzgn95o8wUD"
   },
   "outputs": [],
   "source": [
    "reward_params=[lambd, omega, eta, rho]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "58NX65KOPQRD"
   },
   "outputs": [],
   "source": [
    "# Create a G-learner\n",
    "G_learner = G_learning_portfolio_opt(num_steps,\n",
    "                 reward_params,  \n",
    "                 beta,                \n",
    "                 benchmark_portf,\n",
    "                 gamma, \n",
    "                 num_risky_assets,\n",
    "                 riskfree_rate,\n",
    "                 expected_risky_returns, # array of shape num_steps x num_stocks\n",
    "                 Sigma_r,     # covariance matrix of returns of risky matrix                    \n",
    "                 x_vals_init, # array of initial values of len (num_stocks+1)\n",
    "                 use_for_WM = True) # use for wealth management tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mhy6IPKs7nRq",
    "outputId": "78fbc7d6-2869-48a9-f754-c9ed7c5127c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing G-learning, it may take a few seconds...\n"
     ]
    }
   ],
   "source": [
    "G_learner.reset_prior_policy()\n",
    "error_tol=1.e-8 \n",
    "max_iter_RL = 200\n",
    "G_learner.G_learning(error_tol, max_iter_RL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZJbI_k9mPQQo",
    "outputId": "45fc8ba1-fb6d-450b-8502-b96ffa56419a"
   },
   "outputs": [],
   "source": [
    "num_sim = 1000\n",
    "trajs = []\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "t_0 = time.time()\n",
    "\n",
    "\n",
    "x_vals = [x_vals_init]\n",
    "returns_all = []\n",
    "for n in range(num_sim):\n",
    "    this_traj = []\n",
    "    x_t = x_vals_init[:]\n",
    "    returns_array = []\n",
    "    for t in range(0,num_steps):\n",
    "        \n",
    "        \n",
    "       \n",
    "        mu_t = G_learner.u_bar_prior[t,:] + G_learner.v_bar_prior[t,:].mv(torch.tensor(x_t))\n",
    "        u_t = np.random.multivariate_normal(mu_t.detach().numpy(), G_learner.Sigma_prior[t,:].detach().numpy())\n",
    "        # compute new values of x_t\n",
    "\n",
    "        x_next = x_t +u_t\n",
    "        # grow this with random return\n",
    "        \n",
    "        idiosync_vol =  0.05 # vol_market     \n",
    "        rand_norm = np.random.randn(num_risky_assets)\n",
    "        \n",
    "        # asset returns are simulated from a one-factor model\n",
    "        risky_asset_returns = (expected_risky_returns[t,:] + beta_vals * (returns_market[t] - mu_market * dt) \n",
    "                         + idiosync_vol * np.sqrt(1 - beta_vals**2) * np.sqrt(dt) * rand_norm)\n",
    "        \n",
    "        returns = np.hstack((riskfree_rate*dt, risky_asset_returns))\n",
    "        \n",
    "        x_next = (1+returns)*x_next\n",
    "        port_returns=(x_next.sum() -x_t.sum() -np.sum(u_t) - 0.015*np.abs(u_t).sum())/x_t.sum()\n",
    "        \n",
    "        this_traj.append((x_t, u_t))\n",
    "        \n",
    "        # rename\n",
    "        x_t = x_next\n",
    "        returns_array.append(port_returns) \n",
    "    # end the loop over time steps\n",
    "    trajs.append(this_traj)\n",
    "    returns_all.append(returns_array)\n",
    "\n",
    "print('Done simulating trajectories in %f sec'% (time.time() - t_0))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bpnem5H5vZ5H"
   },
   "source": [
    "### Calculate performance of G-learner (Diagnostics only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Z731JGirsdV"
   },
   "outputs": [],
   "source": [
    "returns_all_G=returns_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UIYUAIWvO04X",
    "outputId": "7e9f5055-b670-4a22-c6cf-d8082c03c9c6"
   },
   "outputs": [],
   "source": [
    "SR_G=0\n",
    "for i in range(num_sim):\n",
    "  SR_G+=(np.mean(returns_all_G[i])-riskfree_rate*dt)/np.std(returns_all_G[i])\n",
    "\n",
    "SR_G/=num_sim\n",
    "print(SR_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TT9dIJP_t0K_"
   },
   "outputs": [],
   "source": [
    "r_G=np.array([0]*num_steps, dtype='float64')\n",
    "for n in range(num_steps):\n",
    "  for i in range(num_sim):\n",
    "    r_G[n]+=returns_all_G[i][n]\n",
    "  r_G[n]/=num_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "id": "KAdjSXoHui7R",
    "outputId": "9f5ea1f7-38b7-4aff-ef5a-e9ef9788651d"
   },
   "outputs": [],
   "source": [
    "plt.plot(r_G, label='G-learning (' + str(np.round(SR_G,3)) + ')', color='red')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('time (quarters)')\n",
    "plt.ylabel('Sample Mean Returns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aTWkeFsDPQQu"
   },
   "outputs": [],
   "source": [
    "np.save('State_act_trajs.npy', trajs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "SNtCvuFMPQQ0",
    "outputId": "2f12bdef-6454-4811-9d4b-5ee128b9f596"
   },
   "outputs": [],
   "source": [
    "#trajs = np.load('State_act_trajs.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Wdu1t058d7a"
   },
   "source": [
    "## GIRL\n",
    "Implement the optimizer for finding the optimal G-learning parameters by MLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gH6Ho4m9yHvX"
   },
   "outputs": [],
   "source": [
    "def get_loss(trajs,\n",
    "             num_steps, \n",
    "             benchmark_portf, \n",
    "             gamma, \n",
    "             num_risky_assets, \n",
    "             riskfree_rate, \n",
    "             expected_risky_returns, \n",
    "             Sigma_r, \n",
    "             x_vals_init, \n",
    "             max_iter_RL, \n",
    "             reward_params,\n",
    "             beta, \n",
    "             num_trajs, \n",
    "             grad=False, \n",
    "             eps=1e-7):\n",
    "\n",
    "\n",
    "  data_xvals = torch.zeros(num_trajs,  num_steps, num_assets, dtype=torch.float64, requires_grad=False)\n",
    "  data_uvals = torch.zeros(num_trajs,  num_steps, num_assets, dtype=torch.float64, requires_grad=False)\n",
    "        \n",
    "  for n in range(num_trajs):\n",
    "        for t in range(num_steps):\n",
    "            data_xvals[n,t,:] = torch.tensor(trajs[n][t][0],dtype=torch.float64)\n",
    "            data_uvals[n,t,:] = torch.tensor(trajs[n][t][1],dtype=torch.float64)\n",
    "                \n",
    "                \n",
    "  # allocate memory for tensors that wil be used to compute the forward pass\n",
    "  realized_rewards = torch.zeros(num_trajs, num_steps, dtype=torch.float64, requires_grad=False)\n",
    "  realized_cum_rewards = torch.zeros(num_trajs, dtype=torch.float64, requires_grad=False)\n",
    "\n",
    "  realized_G_fun = torch.zeros(num_trajs, num_steps, dtype=torch.float64, requires_grad=False)\n",
    "  realized_F_fun  = torch.zeros(num_trajs,  num_steps, dtype=torch.float64, requires_grad=False)\n",
    "\n",
    "  realized_G_fun_cum = torch.zeros(num_trajs, dtype=torch.float64, requires_grad=False)  \n",
    "  realized_F_fun_cum = torch.zeros(num_trajs, dtype=torch.float64, requires_grad=False)  \n",
    "\n",
    "  reward_params_dict={}\n",
    "  loss_dict={}\n",
    "  loss_dict[-1]=np.array([0]*len(reward_params), dtype='float64') # perturb up\n",
    "  loss_dict[1]=np.array([0]*len(reward_params), dtype='float64') # perturb down\n",
    "  loss_grad = np.array([0]*len(reward_params), dtype='float64') \n",
    "\n",
    "  if grad: # compute gradient\n",
    "    for j in range(len(reward_params)):\n",
    "      for k in [-1,1]:\n",
    "            reward_params_dict[k]=reward_params\n",
    "            reward_params_dict[k][j]= reward_params_dict[k][j] + k*eps\n",
    "              \n",
    "            # 1. create a G-learner\n",
    "            G_learner = G_learning_portfolio_opt(num_steps,\n",
    "                                             reward_params_dict[k],\n",
    "                                             beta,\n",
    "                                             benchmark_portf,\n",
    "                                             gamma,\n",
    "                                             num_risky_assets,\n",
    "                                             riskfree_rate,\n",
    "                                             expected_risky_returns,\n",
    "                                             Sigma_r,\n",
    "                                             x_vals_init,\n",
    "                                             use_for_WM=True)\n",
    "        \n",
    "            G_learner.reset_prior_policy()\n",
    "        \n",
    "            # run the G-learning recursion to get parameters of G- and F-functions\n",
    "            G_learner.G_learning(error_tol, max_iter_RL)\n",
    "        \n",
    "            # compute the rewards and realized values of G- and F-functions from \n",
    "            # all trajectories\n",
    "            for n in range(num_trajs):\n",
    "              for t in range(num_steps):\n",
    "                \n",
    "                realized_rewards[n,t] = G_learner.compute_reward_on_traj(t,\n",
    "                                data_xvals[n,t,:], data_uvals[n,t,:])\n",
    "\n",
    "\n",
    "                realized_G_fun[n,t] = G_learner.compute_G_fun_on_traj(t,\n",
    "                                data_xvals[n,t,:], data_uvals[n,t,:])\n",
    "\n",
    "\n",
    "                realized_F_fun[n,t] = G_learner.compute_F_fun_on_traj(t,\n",
    "                                data_xvals[n,t,:])\n",
    "                \n",
    "              realized_cum_rewards[n] = realized_rewards[n,:].sum()\n",
    "              realized_G_fun_cum[n] = realized_G_fun[n,:].sum()\n",
    "              realized_F_fun_cum[n] = realized_F_fun[n,:].sum()\n",
    "          \n",
    "\n",
    "\n",
    "            \n",
    "            loss_dict[k][j] = - beta *(realized_G_fun_cum.sum() - realized_F_fun_cum.sum())\n",
    "      loss_grad[j]=(loss_dict[1][j]-loss_dict[-1][j])/(2.0*eps)\n",
    "\n",
    "  G_learner = G_learning_portfolio_opt(num_steps,\n",
    "                                      reward_params,\n",
    "                                      beta,\n",
    "                                      benchmark_portf,\n",
    "                                      gamma,\n",
    "                                      num_risky_assets,\n",
    "                                      riskfree_rate,\n",
    "                                      expected_risky_returns,\n",
    "                                      Sigma_r,\n",
    "                                      x_vals_init,\n",
    "                                      use_for_WM=True)\n",
    "        \n",
    "  G_learner.reset_prior_policy()\n",
    "        \n",
    "  G_learner.G_learning(error_tol, max_iter_RL)\n",
    "        \n",
    "  # compute the rewards and realized values of G- and F-functions from \n",
    "  # all trajectories\n",
    "  for n in range(num_trajs):\n",
    "      for t in range(num_steps):\n",
    "                \n",
    "                realized_rewards[n,t] = G_learner.compute_reward_on_traj(t,\n",
    "                                data_xvals[n,t,:], data_uvals[n,t,:])\n",
    "\n",
    "\n",
    "                realized_G_fun[n,t] = G_learner.compute_G_fun_on_traj(t,\n",
    "                                data_xvals[n,t,:], data_uvals[n,t,:])\n",
    "\n",
    "\n",
    "                realized_F_fun[n,t] = G_learner.compute_F_fun_on_traj(t,\n",
    "                                data_xvals[n,t,:])\n",
    "                \n",
    "      realized_cum_rewards[n] = realized_rewards[n,:].sum()\n",
    "      realized_G_fun_cum[n] = realized_G_fun[n,:].sum()\n",
    "      realized_F_fun_cum[n] = realized_F_fun[n,:].sum()\n",
    "          \n",
    "    \n",
    "\n",
    "  loss = - beta *(realized_G_fun_cum.sum() - realized_F_fun_cum.sum())   \n",
    "  if grad:\n",
    "    return loss, loss_grad\n",
    "  else:\n",
    "    return loss   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z2cYmHjoTZKM"
   },
   "outputs": [],
   "source": [
    "def fun(x, grad_=False, rescale=1.0, constraint=False):\n",
    "    y=x.copy()\n",
    "    y[0]/=sc[0]\n",
    "    y[1]/=sc[1]\n",
    "    y[2]/=sc[2]\n",
    "    y[3]/=sc[3]\n",
    "    with torch.no_grad():\n",
    "       if grad_==False:\n",
    "           if constraint:\n",
    "            y[0] = y[0]**2\n",
    "            y[1] = 1+y[1]**2\n",
    "            y[2] = 1+y[2]**2\n",
    "            y[3] = 1.0/(1.0+np.exp(-y[3]))\n",
    "\n",
    "           ret = rescale*get_loss(trajs,\n",
    "                                 num_steps, \n",
    "                                 benchmark_portf, \n",
    "                                 gamma, \n",
    "                                 num_risky_assets, \n",
    "                                 riskfree_rate, \n",
    "                                 expected_risky_returns, \n",
    "                                 Sigma_r, \n",
    "                                 x_vals_init, \n",
    "                                 max_iter_RL, \n",
    "                                 y, \n",
    "                                 beta,\n",
    "                                 len(trajs), \n",
    "                                 grad=grad_, \n",
    "                                 eps=1e-7).detach().numpy()\n",
    "\n",
    "           print(ret)\n",
    "           return ret\n",
    "       else:\n",
    "            f, df = get_loss(trajs,\n",
    "                                 num_steps, \n",
    "                                 benchmark_portf, \n",
    "                                 gamma, \n",
    "                                 num_risky_assets, \n",
    "                                 riskfree_rate, \n",
    "                                 expected_risky_returns, \n",
    "                                 Sigma_r, \n",
    "                                 x_vals_init, \n",
    "                                 max_iter_RL, \n",
    "                                 y, \n",
    "                                 beta,\n",
    "                                 len(trajs), \n",
    "                                 grad=grad_, \n",
    "                                 eps=1e-7)\n",
    "            return f.detach().numpy()*rescale, df*rescale/sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JQXvu1HqPQRc"
   },
   "source": [
    "### GIRL (G-learning IRL)\n",
    "Two different optimizers are used to demonstrate the implementation of GIRL. The first approach is gradient free and the second approach uses the gradient of the loss function. \n",
    "#### Gradient free\n",
    "Initialize the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "58IbcwyOTm9S"
   },
   "outputs": [],
   "source": [
    "lambd_0 = 0.002\n",
    "omega_0 = 1.1\n",
    "eta_0 = 1.3 \n",
    "beta_0 = beta \n",
    "rho_0 = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qK1h_-r1UCN8"
   },
   "outputs": [],
   "source": [
    "sc=np.array([1,1,1,1]) # optional re-scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O3A81-tocRuz"
   },
   "outputs": [],
   "source": [
    "x0=np.array([lambd_0, omega_0, eta_0, rho_0])\n",
    "x0=[np.sqrt(x0[0]), np.sqrt(x0[1]-1), np.sqrt(x0[2]-1), np.log(x0[3]/(1-x0[3]))]\n",
    "x0*=sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "XdgT7EKxZVDf",
    "outputId": "77a4f480-7d1b-4c78-f021-75f36e51e47a"
   },
   "outputs": [],
   "source": [
    "# test evaluate the loss function\n",
    "fun(x0, False, 1e-9, constraint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "colab_type": "code",
    "id": "NR9zW91ndDVR",
    "outputId": "aef06036-ee0e-4ab4-cc7f-4ea85627a0a5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Optimize with the Nelder-Mead method\n",
    "res = minimize(fun, x0, method='Nelder-Mead', args=(False, 1e-9, True), options={'disp': True, 'maxiter':50}, tol=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "id": "HRAO1V1_dtfd",
    "outputId": "2059a172-03ae-499d-e929-0e158baa8afd"
   },
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bw2lEM2Tn9tz"
   },
   "outputs": [],
   "source": [
    "# Print parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "D7zhJ8K5ePvA",
    "outputId": "c3025c91-c657-4f33-ed2a-665045ecc375"
   },
   "outputs": [],
   "source": [
    "res.x[0]**2/sc[0], (1+res.x[1]**2)/sc[1], (1+res.x[2]**2)/sc[2], 1.0/(1+np.exp(-res.x[3]))/sc[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1TpYxXmq89QD"
   },
   "source": [
    "#### GIRL (Gradient based)\n",
    "Now separately perform GIRL using a gradient based optimizer. This has the advantage of being more accurate but can be less stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VPZgTZ789R_H"
   },
   "outputs": [],
   "source": [
    "# initialize the parameters\n",
    "lambd_0 = 0.002\n",
    "omega_0 = 1.1\n",
    "eta_0 = 1.3 \n",
    "beta_0 = beta \n",
    "rho_0 = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z9nO_7hHBGai"
   },
   "outputs": [],
   "source": [
    "x0=np.array([lambd_0, omega_0, eta_0, rho_0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MLHLm-uU9XSu"
   },
   "outputs": [],
   "source": [
    "# rescaling\n",
    "sc=np.array([1000,0.1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "yEO1I-P2DpLp",
    "outputId": "17c8508d-1071-40fe-9735-2a6d37ae705c"
   },
   "outputs": [],
   "source": [
    "# test evaluation of the loss function\n",
    "x_ask=np.array([lambd, omega, eta, rho])*sc\n",
    "fun(x_ask, False, 1e-6, constraint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WCZdikslULik"
   },
   "outputs": [],
   "source": [
    "# choose bounds for parameters\n",
    "bnds=((0.0001*sc[0], 0.0025*sc[0]), (0.01*sc[1], 1.5*sc[1]), (1.001*sc[2],2*sc[2]), (0.01*sc[3],1*sc[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "yScEwb3xUcSc",
    "outputId": "597e90db-1c56-4f6a-81d5-3ea9e9a26d5e"
   },
   "outputs": [],
   "source": [
    "# L-BFGS-B for gradient solver with bounds\n",
    "res = minimize(fun, x0, method='L-BFGS-B', bounds=bnds, args=(False, 1e-6, False), options={'disp': True, 'maxiter':50}, tol=0.0000001) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "colab_type": "code",
    "id": "_6APDqOx-Tqx",
    "outputId": "41710a5d-050d-46fc-b135-659b64d74da2"
   },
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9TcGgfihDQG1",
    "outputId": "16c1069e-f90d-443f-f897-87c7d9ea3892"
   },
   "outputs": [],
   "source": [
    "#print results. Note that these may differ from the actual G-learner parameters depending on the optimizer. \n",
    "# The optimizer will attempt to find the closest set of parameters.\n",
    "res.x/sc"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "Wealth_Management_GIRL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
